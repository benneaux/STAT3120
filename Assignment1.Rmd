---
title: "<center> STAT3120 Applied Bayesian Methods </center>"
author: "Benjamin G. Moran; <right> c3076448@uon.edu.au </right>"
date: "22nd August 2016"
output:
  tufte::tufte_html:
    highlight: pygments
    smart: TRUE
    graphics: yes
    citation_package: natbib
    latex_engine: xelatex
    includes:
      before_body: eqnnumber.js
    mathjax: default
  pdf_document: default
  word_document: default
subtitle: <center> Semester 2, 2016 </center>
---

```{r setup, include=FALSE}
require(tufte)
require(knitr)
require(reshape2)
opts_chunk$set(cache =TRUE)
opts_chunk$set(collapse = TRUE, highlight = TRUE)
require(ggplot2)
require(broom)
require(magrittr)
require(dplyr)
```

Due Monday 22 August 2016 (on-campus by 3pm, distance by 5pm)

Late assignments will not be accepted without prior written permission of the lecturer.

***

###Total marks: 50

Marking Criteria:

1.	Answers must be written in clear English with all appropriate working and/or supporting computer output shown.
2.	Raw computer output without explanatory text is unacceptable.
3.	Students are required to understand the content in Weeks 1-4 to answer these questions.
4.	As part of your workings you should include applicable R-code.

***
 
#Question 1 Total: 15 marks

**Law case (taken from Rossman and Short, 1995, Journal of Statistics Education).**

Joseph Jameison was charged with multiple rapes in 1987 in Allegheny County in the US. DNA evidence from the scenes of the crimes revealed that the attacker displayed the genetic marker PGM2+1- in his DNA. This marker has only a 0.32% prevalence in the general population.

###(a) Discuss how you might choose a prior probability that Jameison is guilty. Explain and defend your choice (do not set a specific number yet). 2 marks

**Answer:** From the description of the case we should highlight two important pieces of information. Firstly, the percentage of the population that carry the genetic marker displayed by the attacker is *very* small - $0.32\%$. Next, we should note what is omitted from the description of the genetic marker evidence: no values for the sensitivity or specificity of the test that determined the presence of the marker are stated. Therefore - at least in the toy context of this problem - we will assume that those values are both $100\%$. Obviously in reality this is impossible and we would have to take into account the accuarcy of the method/test used.

So, in summary, we are told that:

  + The marker is *definitely* present.
  + The attacker displayed the marker.
  + Joseph Jameison is a suspect. 

We are not given any reason's for why he is a suspect, so the only way we can decide on his guilt is by determining whether or not he displays the same genetic marker. This is an either/or question so it makes sense - without more evidence - to assign a $50 / 50$ probabily of guilt. However, we should also take into account that the general principle of an assumption of innocence is a key tenet of most modern legal systems (certainly those in the anglosphere). Therefore it seems reasonable to suggest an intial probability of guilt $Pr(guilty) < 0.5$.

###(b) Update your prior with the observed information. Display the full formula for Bayesâ€™ rule in terms of the as yet unknown prior Pr(guilty). 3 marks

**Answer:** We are looking for the folliwing formula:

$$Pr(G \mid M) = \frac{Pr(M \mid G) \, Pr(G)}{Pr(M \mid G) \, Pr(G) + Pr(M \mid G^{c}) \, Pr(G^{c})}$$

Where

$$\begin{aligned}
G &:= \text{Joseph Jameison (JJ) is Guilty} \\
M &:= \text{An individual carries the PGM2+1- marker} \\
Pr(M \mid G) &:= \text{JJ carries the PGM2+1- marker, given that he is guilty} \\
Pr(M \mid G^{c}) &:= \text{JJ carries the PGM2+1- marker, given that he is not guilty}
\end{aligned}$$

Now, given that the question states that the attacker *definitely has the marker*, the probability that JJ has the marker given that he is guilty is $Pr(M \mid G) = 1$.

The probability that JJ has the marker given that he is not guilty is $Pr(M \mid G^{c}) = (total  population  *  prevalence of the marker) - 1 / total population \approx 0.32\%$. So now we can restate the equation for Bayes rule as it applies here:

$$\begin{aligned} Pr(G \mid M) &= \frac{Pr(M \mid G) \, Pr(G)}{Pr(M \mid G) \, Pr(G) + Pr(M \mid G^{c}) \, Pr(G^{c})} \\
Pr(G \mid M) &= \frac{(1) \, Pr(G)}{(1) \, Pr(G) + (0.0032) \, Pr(G^{c})} \\
Pr(G \mid M) &= \frac{Pr(G)}{Pr(G) + (0.0032) \,*\, Pr(G^{c})}
\end{aligned}$$

###(c) Display the posterior probability of guilt using a range of suitable values for the prior Pr(guilty). 3 marks

```{r Q1ba}
pr.guilty <- seq(0.1,1,by=0.1)
pr.marker <- 0.0032
pr.notguilty <- 1-pr.guilty
pr.post <- (1*pr.guilty)/(1*(pr.guilty) + pr.marker*(pr.notguilty))
prob.dens <- as.data.frame(cbind(pr.guilty, pr.post)) %>%
  transmute("Pr(Guilt)" =  pr.guilty, Posterior = signif(pr.post, digits = 5))

prob.dens
```

Which doesn't shed much light on how the probability changes, other than telling us that it is above $90\%$ for most values we can assign to $Pr(Guilt)$. Instead, lets simulate some more and plot the results. Note that the red line represents $97.5\%$ certainty, which i'll refer to in part (d).

```{r Q1b2}
pr.guilty <- seq(0,1,by=0.0001)
pr.marker <- 0.0032
pr.notguilty <- 1-pr.guilty
pr.post <- (1*pr.guilty)/(1*(pr.guilty) + pr.marker*(pr.notguilty))
prob.dens <- as.data.frame(cbind(pr.guilty, pr.post))
```

```{r Q1b plot, fig.margin = FALSE, fig.fullwidth = TRUE, fig.align='center'}
ggplot2::ggplot(prob.dens, 
                aes(x=pr.guilty,y=pr.post)
                ) +
  geom_line(data=prob.dens) +
  geom_hline(yintercept=0.975, 
             linetype = 2, 
             color = "red"
             )
```

***

###(d) Say that a jury wants to be at least 97.5% sure of guilt before returning a guilty verdict. What is the smallest prior probability of guilt that could be chosen to ensure this level of probability. 2 marks

We have shown above that the majority of prior probabilities we could assign would satisfy this criteria. In order to find a minimum value however, we'll need to sample from the distribution. First, we need to find the index of the first of the probabilities we calculated for the posterior density that is greater than $0.975$. Then we find the corresponding value for the probabilty of guilt that generated the value at that index.

```{r Q1d}
mpr.guilt <- which(pr.post > 0.975)
pr.guilty[mpr.guilt[1]]
```

Therefore, the minimum probability of guilt that the can be chosen is $\approx 11.1\%$.

###(e)	Suppose there are 4,622 males living in Allegheny County. Given that we have found 1 male with PGM2+1- in his DNA, what is the probability that other males in the county also display this marker?	3 marks

**Answer:** We are not told that the distribution of the genetic marker amongst a population varies by sex. Therefore, we should assume that the $0.32\%$ prevalence stated in the question is suitable to use here. In a population of $4,622$ we would expect to see:

```{r Q1e}
n <- 4622; prev <- 0.0032;
pr.another <- ((n-1)*prev)/(n*prev)
pr.another
```

So there is approximately a $99.98\%$ chance that, given there is one male with the marker, there will be at least one more.

###(f) Should Jameison be found guilty on the DNA evidence alone? Discuss.	2 marks

**Answer:** If we generate data for $Pr(Guilt)$ values between $0$ and $1\%$ we can see that they span a large range of values.
```{r Q1f, echo = FALSE}
pr.guilty <- seq(0,.01,by=0.001)
pr.notguilty <- 1-pr.guilty
pr.post <- (1*pr.guilty)/(1*(pr.guilty) + pr.marker*(pr.notguilty))
prob.dens <- as.data.frame(cbind(pr.guilty, pr.post))
prob.dens
```

When a prior probability this small is selected, we should be wary. Returning to part (e), if we were to take our prior as $1-Pr(>1 \mid 1) = 1 - 0.9997836 = 0.0002164$, the posterior probability would be:
```{r q1f2, echo = FALSE}
pr.guilty <- 0.0002164
pr.notguilty <- 1-pr.guilty
pr.post <- (1*pr.guilty)/(1*(pr.guilty) + pr.marker*(pr.notguilty))
prob.dens <- as.data.frame(cbind(pr.guilty, pr.post))
prob.dens
```
or $\approx 6\%$, which is hardly enough evidence to be confident that JJ is guilty. This makes sense as a prior, given that the only piece of evidence that we can test to determine guilt is the presence of the genetic marker. They key piece of information we need to determine is how many people could have committed the crime *other* that JJ - assuming a presumption of innocence. The probability we need is the likelihood that *at least one* other male in the pool of possible suspects - i.e. males in Allegheny County - carries the gene, which we calculated above. The prior probability of JJ being guilty should be set as the complement of this probability - i.e. $0.0002164$. Given the posterior probability returned by this prior, we should conclude that there is **not** enough evidence to convict him on the basis of genetic marker expression alone.

\pagebreak

#Question 2	Total : 10 marks

##Part 1:
ABC book distributors have a new book that they hope will be popular with their customers. To obtain an idea of how popular it will be they choose a random sample of 40 customers, ring them, give them a brief description and ask whether they would like to buy the book. 15 of the sample decided to buy the book. What inference can we make about the true proportion of the population of customers that will buy the new book based on this sample?

###(a) Under a uniform prior, derive the posterior distribution for the unknown true proportion. 2 marks

**Answer:** First of all, what is described corresponds to a Binomial distribution.

$$\begin{aligned} p(\theta | y = 15) &\propto Pr(y = 15 | \theta)p(\theta) \\
&\propto \theta^{15}(1-\theta)^{25}
\end{aligned}$$

We know that the Bayes-Laplace ($Beta(1,1)$) prior approximates a Uniform, flat prior over the interval $(0,1)$. We also know that that the $Beta(\alpha, \beta)$ is a good substitute for the Binomial and will give us a $Beta$ posterior.

So the posterior works out to be

```{r Q2 - 1a}
x <- seq(0,1,by = 0.01); a = 15; b = 40
dens <- dbeta(x,a + 1,b - a + 1)
posterior.dens.unif <- as.data.frame(cbind(x,dens))
ggplot(data = posterior.dens.unif, aes(x,dens)) +
  geom_line()
```

###(b) Is ABC likely to achieve 50% of their population of customers buying the new book? 2 marks

**Answer:** We can use the **pbeta** function to estimate the probability that ABC will meet their target.
```{r 1}
a = 15; b = 40
pbeta(0.5,a + 1,b - a + 1,lower.tail=FALSE)
```

implying that it is not very likely that ABC will achieve their target. We can confirm this be calulating the mean of a sample from the same distribution.

```{r 2}
x <- seq(0,1,by = 0.01)
unif.data <- rbeta(10000,a + 1,b - a + 1)
mean(unif.data)
```

which implies that the most likely outcome for ABC - using the given prior and likelihood - is that $\approx 38\%$ of their customers will purchase the book.

We could also use **qbeta** to find a $90\%$ CI.

```{r}
beta.ci <- qbeta(c(0.95,0.05),a + 1, b - a + 1,lower.tail = FALSE)
names(beta.ci) <- c("5%","95%")
beta.ci
```

which backs up the result above.

##Part 2:
In past ABC new book promotions they have found that on average $23.5\%$ of their customers buy the book being promoted. In fact, the proportion of customers buying new books after phone promotions has a histogram that closely matches a $Beta(4,13)$ density.

###(c) Can we use this as a prior density? Discuss.	2 marks

**Answer:** We can check this by generating some random samples from a $Beta(4,13)$ distribution and calculating the mean of those samples. I'll also plot a histogram of the sample data with the mean superimposed as a red line.

```{r Q2 2c}
a = 3; b = 15
beta.data <- as.data.frame(rbeta(10000,a + 1,b - a + 1))
ggplot(beta.data, aes(beta.data, ..density..)) + 
  geom_histogram(bins = 20) + 
  scale_x_continuous() + 
  geom_vline(xintercept = mean(beta.data[,1]), color = "red")
mean(beta.data[,1])
```

Which is $\approx 23.5\%$. Therefore, we can justifiably use $Beta(4,13)$ as a prior distribution.

###d) Assume the answer in (c) is yes and derive the corresponding posterior. 2 Marks

**Answer:** Assuming a $Beta(4,13)$ prior and a $Beta(13.26)$ Likelihood we are left with a $Beta(19,38)$ The following code will generate our posterior density and plot it.

```{r Q22d}
x <- seq(0,1,by = 0.01); a1 <- 15; b1 <- 40; a2 <-  3; b2 <-  15
dens <- dbeta(x,(a1 + a2) + 1,(b1 + b2) - (a1 + a2) + 1)
posterior.dens.beta <- as.data.frame(cbind(x, dens))
ggplot(data = posterior.dens.beta,
       aes(x,dens)
       ) + 
  geom_line()
```

###(e) How did this prior information affect the likelihood of at least $50\%$ of ABCs population of customers buying the new book?	2 marks

**Answer:** Let's plot both together, with the density derived from the Uniform prior in blue.

```{r Q2e}
ggplot() + 
  geom_line(data = posterior.dens.unif,
            aes(x,dens),color = "blue"
            ) + 
  geom_line(data = posterior.dens.beta,
            aes(x,dens),color = "red"
            )
```

Now lets's look at some summary statistics and also the range in detail:

```{r Q22e2, echo=FALSE}
a1 <- 15; b1 <- 40; a2 <-  3; b2 <-  15
beta.post <- rbeta(1000,(a1 + a2) + 1,(b1 + b2) - (a1 + a2) + 1)
unif.post <- rbeta(1000,a1 + 1,b1 - a1  + 1)
summary <- cbind(summary(unif.post),summary(beta.post))
range <- cbind(range(summary(unif.post)),range(summary(beta.post)))

range.diff <- c(range(summary(unif.post))[2]-range(summary(unif.post))[1],range(summary(beta.post))[2]-range(summary(beta.post))[1])
range <- rbind(range,range.diff)
colnames(summary) <- c("Unif","Beta")
colnames(range) <- c("Unif","Beta")
rownames(range) <- c("Lower","Upper","Diff")
summary
range
```

From the above we can see that the mean is slightly less in the Beta data compared to the Unif data. Also the range is smaller in the Beta data.

We can conclude that the Beta prior provides a tighter estimate of the parameter of interest. It also shows that the parameter is most likely to be $< 50\%$, and we are now surer of this using the Beta prior than we were using the Unif prior. Using the Beta prior has shifted the mean lower so it has negatively affected the likelihood of $50\%$ of ABC's customer population buying the book.

***
 
#Question 3	Total: 4 marks

Suppose you have a Beta(6,6) prior on the probability $\theta$ that a coin will yield a head when spun in a specified manner. The coin is independently spun 5 times. All you are told is that all 5 results were the same, i.e. either 5 heads or 5 tails. Derive your posterior density (up to a proportionality constant) for $\theta$ and sketch it.

**Answer:** We are given that the prior probability of $\theta$ is proportional to a $Beta(6,6)$ distribution:
$$f(\theta) \propto \theta^{5}(1-\theta)^5$$

```{r Q3a}

```

We are also given that the data could represent only two possible outcomes - either all heads or all tails. Therefore, the likelihood of the data is equal to:

$$\begin{aligned} Pr(x \mid \theta) &= \binom{5}{0}(1-\theta)^5 + \binom{5}{5}\theta^5 \\ 
&= (1-\theta)^5 + \theta^5
\end{aligned}
$$

```{r Q3b}

```

So, we can derive the posterior by multiplying the prior by the likelihood:
$$\begin{aligned} 
f(\theta|x) &\propto Pr(x|\theta)f(\theta) \\
&\propto \theta^5(1-\theta)^5*\left((1-\theta)^5 + \theta^5\right) \\
&\propto \theta^5(1-\theta)^{10} + \theta^{10}(1-\theta)^5
\end{aligned}$$
Now let's sketch the distribution

```{r Q3 Data, echo = FALSE}
#24024
theta <- seq(0,1,0.01)
prior <- data.frame(Dist = "Prior", Theta = theta, Dens = theta^5*(1-theta)^5)
like  <- data.frame(Dist = "Likelihood", Theta = theta, Dens = (1-theta)^5 + theta^5)
posterior <- data.frame(Dist = "Posterior", Theta = theta, Dens = theta^5*(1-theta)^10 + theta^10*(1-theta)^5)
coin.data <- as.data.frame(rbind(prior,like, posterior))

```

```{r Q3 plots, fig.margin = FALSE, fig.fullwidth = TRUE}
p <- ggplot(coin.data, aes(x=Theta, y=Dens)) +
  geom_line()
p + facet_grid(Dist ~ ., scales = "free_y")
```

***

#Question 4	Total: 9 marks

A treatment vs control group example discussed at various times in the literature is based on 11 successes in 11 treatments (i.e. 11/11), and a single failure in the control group (i.e. 0/1).

###(a) Find a classical solution to the question of whether the treatment is successful. Bare output from a statistical package is acceptable, but without understanding the
procedure, it may be difficult to obtain full marks at (c).	3 marks

**Answer:** After putting the values given in the question into 2x2 contingency table, we can use the **chisq.test** function in **R** to run some classical analysis.

```{r Q4 classical, echo = FALSE}
group <- c("Treatment","Treatment","Control","Control")
outcome <- c("Success","Failure","Success","Failure")
n <- c(11,0,0,1)
data <- data.frame(group,outcome,n)
tab <- xtabs(n~group+outcome, data = data)
tab
Xsq <- broom::glance(chisq.test(tab, correct = FALSE))
Xsq
```

Here we find that the p-value returned by the $\chi^2$ test is $\approx$ $`r as.numeric(Xsq$p.value)`$ $< \alpha = 0.05$, which implies that we should reject $H_{0}$: that the treatment result is not significant. However, we should note the warning that is also returned along with the result: these tests are succeptible to error when extreme data - i.e. 1 or 0 successes/failures - is used.

Note also that if we were to try and compute summary statistics using Classical methods, all of the key measures - Upper, Lower, Mean, Median, Mode - would $=1$ and the standard deviation $=0$, which is clearly unsatisfactory. Classical methods break down when the probabilities involved are either equal to $1$ or $0$, as is the case  in this example.

###(b) Apply a Bayesian approach to this question, trying both the Bayes-Laplace and Jeffreys priors.	4 marks

**Answer:** We should define two Binomial distributions, one for each treatment group: "Treatment" and "Control". 

$$\begin{aligned} X_{Treatment} &\sim Binomial(11, \theta_{T}) \\
X_{Control} &\sim Binomial(1, \theta_{C})
\end{aligned}$$

We know that a good conjugate prior to use for the Binomial distribution is a Beta distribution. So let's define two conjugate priors for our data.

$$\begin{aligned} Pr(\theta_{T}) &\sim Beta(\alpha_{T}, \beta_{T}) \\
Pr(\theta_{C}) &\sim Beta(\alpha_{C}, \beta_{C})
\end{aligned}$$

Our posterior distributions for each would then both be Beta distributions:

$$\begin{aligned} Pr(\theta_{T}|X_{T}) &\sim Beta(\alpha = 11 + \alpha_{T},\beta = 0 + \beta_{T}) \\
Pr(\theta_{C}|X_{C}) &\sim Beta(\alpha = 0 + \alpha_{C},\beta = 1 + \beta_{C})
\end{aligned}$$

Now, a Bayes-Laplace prior $\equiv Beta(1,1)$. Assigning this to both of our distributions gives us two posterior distributions.
$$\begin{aligned} Pr(\theta_{T}|X_{T}) &\sim Beta(\alpha = 11 + 1,\beta = 0 + 1) &\implies Beta(12,1) \\
Pr(\theta_{C}|X_{C}) &\sim Beta(\alpha = 0 + 1,\beta = 1 + 1) &\implies Beta(1,2)
\end{aligned}$$

**Bayes-Laplace Priors**

```{r Q4 BL Data, echo = FALSE}
a = 11; b = 0; 
post.treat.bl <- data.frame(Dist = "Treat", Dens = rbeta(1000,a + 1,b + 1))
a = 0; b = 1
post.control.bl <- data.frame(Dist = "Control", Dens = rbeta(1000,a + 1,b + 1))
post.comb <- post.control.bl$Dens*post.treat.bl$Dens
post.bl <- data.frame(Dist = "Posterior", Dens = post.comb)
bl.density <- rbind(post.treat.bl,post.control.bl, post.bl)
```

```{r Q4 BL Plots, echo= FALSE, fig.margin = FALSE, fig.fullwidth = TRUE}
ggplot(bl.density, aes(Dens)) + 
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous() +
  facet_grid(. ~Dist) + 
  theme_light()
```

A Jeffrey's prior however is $\equiv Beta(1/2,1/2)$. Assigning this to both of our distributions gives us two more posterior distributions.

$$\begin{aligned} Pr(\theta_{T}|X_{T}) &\sim Beta(\alpha = 11 + 0.5,\beta = 0 + 0.5) &\implies Beta(11.5,0.5) \\
Pr(\theta_{C}|X_{C}) &\sim Beta(\alpha = 0 + 0.5,\beta = 1 + 0.5) &\implies Beta(0.5,1.5)
\end{aligned}$$

**Jeffrey's Priors**

```{r Q4 J Data, echo = FALSE}
a = 11; b = 0
post.treat.j <-data.frame(Dist = "Treatment", Dens = rbeta(1000,a + 0.5,b + 0.5))
a = 0; b = 1
post.control.j <- data.frame(Dist = "Control", Dens = rbeta(1000,a + 0.5,b + 0.5))
post.comb <- post.treat.j$Dens*post.control.j$Dens
post.j <- data.frame(Dist = "Posterior", Dens = post.comb)
j.density <- rbind(post.treat.j,post.control.j, post.j)
```

```{r Q4 J Plots, echo = FALSE, fig.margin = FALSE, fig.fullwidth = TRUE}
ggplot(j.density, aes(Dens)) + 
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous() +
  facet_grid(. ~Dist) +
  theme_light()

```


###(c) Briefly explain possible reasons behind the various differences.	2 marks

**Answer** First, let's compile some summary statistics for the posterior densities.

```{r Q4c, echo= FALSE}

sum.j <- broom::tidy(post.j)

sum.bl <- broom::tidy(post.bl)
summ <- rbind(sum.bl[,5:length(sum.bl)-3], sum.j[,5:length(sum.j)-3])
summ <- subset(as.data.frame(summ), select = c(-trimmed,-mad, -n))

rownames(summ) <- c("BL", "Jeffrey's")
summ
```

As mentioned in part (a) the corresponding values using classical methods would be non-sensical in these extreme cases (i.e. all statistics equal to 1). So here we have clear evidence for why we should prefer Bayesian methods in these instances: they provide us with usable statistics to work with.

There are some differences in the shape of the two posterior Bayes-derived distributions, which I believe is down to the fact that the BL prior is effectively flat, whilst the Jeffery's prior is u-shaped. The result of this is that BL prior is dominated more by the data in the control group and thus changes the shape of the posterior very little. The Jeffrey's prior on the other hand puts a greater emphasis

***

#Question 5	Total: 4 marks

A random sample of n students is drawn from a large population, and their weights are measured. The average weight of the n sampled students is $\bar{y} = 150$ pounds. Assume the weights in the population are Normally distributed with unknown mean $\theta$ and known standard deviation of 20 pounds. Suppose your prior distribution for $\theta$ is Normal with mean 180 and known standard deviation of 40 pounds.


Give your posterior distribution for $\theta$ . (Note that your answer will be a function of n.)

We are given the following:
$$\begin{aligned} \bar{y}\mid \theta &\sim N\left(\mu,\frac{20^{2}}{n}\right) \\
\theta &\sim N\left(180, 40^{2}\right)
\end{aligned}
$$
So we are looking for the following:

$$Pr(\theta \mid \bar{y}) = N(\theta \mid \mu_{n}, \tau_{n}^{2})$$

From class we know that 

$$\begin{aligned}
\mu_{n} &= \frac{\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{n \bar{y} }{\sigma^{2}}}{\frac{1}{\tau_{0}^{2}} + \frac{n}{\sigma^{2}}} \\
\frac{1}{\tau_{n}^{2}} &= \frac{1}{\tau_{0}^{2}} + \frac{n}{\sigma^{2}}
\end{aligned}$$
So, we end up with:
$$\begin{aligned}Pr(\theta \mid \bar{y}) &= N\left(\theta \mid \frac{\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{n\bar{y}}{\sigma^{2}}}{\frac{1}{\tau_{0}^{2}} + \frac{n}{\sigma^{2}}}, \frac{1}{\frac{1}{\tau_{0}^{2}} + \frac{n}{\sigma^{2}}}\right) \\
Pr(\theta \mid \bar{y}) &= N\left(\theta \mid \frac{\frac{180}{40^{2}} + \frac{150n}{20^{2}}}{\frac{1}{40^{2}} + \frac{n}{20^{2}}},\frac{1}{\frac{1}{40^{2}} + \frac{n}{20^{2}}}\right) 
\end{aligned}$$


***
 
#Question 6	Total: 8 marks

Consider the Uniform(0, $\theta$) distribution, i.e. 
$$p(x \mid \theta) = \frac{1}{\theta},	0 < x < \theta$$

from which a random sample $\{x_1,...,x_n\}$ is obtained. Note that $y_n = \text{max}\{x_1,...,x_n\}$ is a sufficient statistic.

###(a) Derive the posterior distribution $p(\theta | x_1,...,x_n) = p(\theta | y_n)$, based on the noninformative prior $p(\theta) \propto 1/ \theta$.
Be careful to use the correct likelihood and range!	3 marks

**Answer:** We know the Joint Density of $\theta_{i} \sim \mathcal{U}[0,\theta], 1 \leq i \leq n$ is what we need for our likelihood, so we can write our Likelihood as:

$$L(\theta \mid x) \propto \begin{cases}\prod\limits_{i=1}^{n} \frac{1}{\theta^{i}} & \text{max}\>\left\{x_1, x_2, \cdots, x_n\right\} \leq \theta \\ 0 & otherwise 
\end{cases}$$

with a constant of integration equal to:
$$\begin{aligned} \int\limits_{y_{n}}^{\infty}Pr(y_n \mid \theta) \, Pr(\theta) d\theta &=
\int\limits_{y_{n}}^{\infty} \frac{1}{\theta^n} \, \frac{1}{\theta} d\theta \\
&= \int\limits_{y_{n}}^{\infty} \frac{1}{\theta^{n+1}} d\theta \\
&= \left[-\frac{1}{n\theta^{n}}\right]_{y_{n}}^{\infty} \\
&= \left[0 -\left(-\frac{1}{ny_{n}^{n}}\right)\right] \\
&= \frac{1}{ny_{n}^{n}}
\end{aligned}$$

We also know that $Pr(\theta \mid y_{n}) = \> \frac{Pr(y_{n} \mid \theta) \, Pr(\theta)}{1/ny_{n}^{n}}$, which implies that our posterior is given by:

$$Pr(\theta \mid x) = \begin{cases}\frac{ny_{n}^{n}}{\theta^{n+1}} & \text{max}\>\left\{x_1, x_2, \cdots, x_n\right\} \leq \theta \\ 0 & otherwise 
\end{cases}$$


###(b) Plot this posterior for $y_n = 5$ and $n = 20$.	2 marks

**Answer:** With $y_n = \text{max}\>\left\{x_1, x_2, \cdots, x_n\right\} = 5$ and $n = 20$ we have a posterior distribution of 
$$Pr(\theta \mid x) = \begin{cases} \frac{20*5^{20}}{\theta^{21}} & \text{max}\>\left\{x_1, x_2, \cdots, x_{20}\right\} \leq \theta \\ 0 & otherwise \end{cases}$$
Let's plot it with a red line representing $y_{n} = 5$.

```{r Q6b Data}
y.n <- 32.2; n <- 20;
theta <- seq(y.n,50, by = 0.001)
post.dens <- (19*((y.n)^{19}))/(theta^(n))
dens <- as.data.frame(cbind(theta,post.dens))
theta.0 <- as.data.frame(seq(0,10, by = 0.001))
```

```{r Q6b Plot, echo = FALSE, fig.margin = FALSE}
ggplot2::ggplot() + 
    geom_line(data = dens,aes(theta,post.dens)) + 
    geom_vline(xintercept=y.n,color= "red",linetype = 2) +
    geom_segment(data = theta.0,x=0,xend=y.n,y=0,yend=0) +
    geom_hline(yintercept = 0.05) +
    xlim(0,50)
```

From this it should be obvious that the MLE of $\theta$ is achieved whene $\theta = y_n$.

###(c) For the data at (b), derive 95% credible intervals, both central and HPD. Which would you prefer and why? 3 marks

**Answer:** We calculate the upper bound - only the upper bound - of the HPD by $\forall \alpha \in [0,1]: HPD(\theta \mid y_{n}) = (y_{n}, \alpha^{-(n-1)} \, y_{n})$, which for $\alpha = 0.05$ equals:

```{r Q6c}
y.n <- 5; alph <- 0.05; n <- 20
HPD.l <- y.n; HPD.u <- ((alph)^(-1/(n-1)))*y.n; HPD <- c(HPD.l, HPD.u)
HPD
```
If we assume that we want a Central Interval corresponding to $[0.025, 0.975]$ then it will not include the MLE, which by default makes us favour the HPD. If we take the interval $[0,0.95]$ then it will include the MLE and be narrower - in this instance - than the HPD.

```{r Q6last}
ind <- c(quantile(post.dens, c(0.95))[[1]])
ind2 <- c(which(post.dens < ind)[[1]])
u.int <- theta[ind2]
CL <- c(y.n,u.int)
```
However, given that this is not proper, I would prefer the HPD for the reason stated above.
***

####End of Assignment 1
