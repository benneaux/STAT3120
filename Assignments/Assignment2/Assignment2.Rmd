---
title: "<center> STAT3120 Applied Bayesian Methods </center>"
author: "Benjamin G. Moran; <right> c3076448@uon.edu.au </right>"
date: "20th September 2016"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    graphics: yes
    highlight: pygments
    includes:
      before_body: eqnnumber.js
    mathjax: default
    smart: yes
subtitle: <center> Semester 2, 2016 </center>
---

```{r setup, include=FALSE}
require(tufte)
require(knitr)
require(dplyr)
require(reshape2)
require(tidyr)
opts_chunk$set(cache = FALSE)
opts_chunk$set(fig.align = 'center', fig.height = 6, fig.width = 12, collapse = TRUE, highlight = TRUE)
```

Due Friday $16^{th}$ September 2016 (on-campus by 3pm, distance by 5pm)

Late assignments will not be accepted without prior written permission of the lecturer.

***

###Total marks: 45

Marking Criteria:

1.	Answers must be written in clear English with all appropriate working and/or supporting computer output shown.
2.	Raw computer output without explanatory text is unacceptable.
3.	Students are required to understand the content in Weeks 2-7 to answer these questions.
4.	As part of your workings you should include applicable R-code.

***
 
##Question 1 Total: 12 marks

(a)	Consider a sample of $n=15$ data points, $y$, from the Normal distribution with unknown mean $\mu$ and variance $\sigma^{2}$ . For this sample we wish to allow the 14th and 15th data points, $(y_{14}, y_{15})$, to have an inflated variance, namely $10 \sigma^{2}$ .

    (i) Write down the likelihood $p( y \mid \mu,\sigma^{2})$ , including all constants. [2 marks]
    
    **Answer:** The likelihood is:
    
    $$
      \begin{aligned}
        p(y \mid \mu, \sigma^{2}) 
          &= \prod
               \limits_{i=1}^{13}
             (2\pi\sigma^{2})^{-\frac{1}{2}}
             exp
              \left\{
                -\frac{1}{2\sigma^{2}}
                (y_{i}-\mu)^2
              \right\}
             \times
             \prod\limits_{i=14}^{15}
             (20\pi\sigma^{2})^{-\frac{1}{2}}
             exp
              \left\{
                -\frac{1}{20\sigma^{2}}
                (y_{i}-\mu)^{2}
              \right\} \\
          &= (2\pi\sigma^{2})^{-\frac{13}{2}}
             exp
              \left\{
                -\frac{1}{2\sigma^{2}}
                \sum\limits_{i=1}^{13}
                  (y_{i}-\mu)^2
              \right\}
             \times
             (20\pi\sigma^{2})^{-\frac{2}{2}}
             exp
              \left\{
                -\frac{1}{20\sigma^{2}}
                \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}
              \right\} \\
          &= 20^{-1}(2)^{-\frac{13}{2}}(\pi\sigma^{2})^{-\frac{15}{2}}
             exp
              \left\{
                -\frac
                {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
                - \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}}
                {20\sigma^{2}}
              \right\} \\
          &\propto (\sigma^{2})^{-\frac{15}{2}}
             exp
              \left\{
                -\frac
                {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
                - \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}}
                {20\sigma^{2}}
              \right\}
      \end{aligned}
    $$
    
To perform inference on $p(\mu,\sigma^{2} \mid y)$ we partition using $p(\mu \mid y)$ and $p(\sigma^{2} \mid y, \mu)$.
        
(ii) Mathematically derive the density  $p(\sigma^{2} \mid y, \mu)$ assuming the non-informative prior $p(\sigma^{2} ) \propto 1/\sigma^{2}$ . Derive $E(\sigma^{2} \mid y, \mu)$ and $V(\sigma^{2} \mid y, \mu)$. [4 marks]
    
    **Answer:** We can express the joint posterior density as:
    
    $$
      \begin{aligned}
        p(\sigma^{2} \mid y, \mu) 
          &\propto p(y \mid \mu, \sigma^{2})\,p(\sigma^{2}) \\
          &\propto (\sigma^{2})^{-\frac{15}{2}}
             exp
              \left\{
                -\frac
                {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
                - \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}}
                {20\sigma^{2}}
              \right\}
            \times
            \frac
              {1}{\sigma^{2}} \\
          &\propto (\sigma^{2})^{\left(-\frac{15}{2} + 1\right)}
             exp
              \left\{
                -\frac
                {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
                - \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}}
                {20\sigma^{2}}
              \right\}          
      \end{aligned}
    $$
Therefore, the density of $p(\sigma^{2} \mid y, \mu)$ has the form of an Inverse Gamma, such that:

$$
  \sigma^{2} \mid y, \mu \sim I.G.\left(\alpha = \frac{15}{2}, \beta = \frac
                {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
                + \sum\limits_{i=14}^{15}
                  (y_{i}-\mu)^{2}}
                {20}
                \right)
$$

We know that the mean of an Inverse Gamma is given by $\frac{\beta}{\alpha -1}$ and the variance by $\frac{\beta^{2}}{(\alpha - 1)^{2}(\alpha - 2)}$. Therefore

$$
  \begin{aligned}
    E(\sigma^{2} \mid y, \mu) 
      &= \frac
          {\frac
            {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
              + \sum\limits_{i=14}^{15}
              (y_{i}-\mu)^{2}}
            {20}}
          {\frac
            {15}{2} - 1
          } \\
     &=\frac
         {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
           + \sum\limits_{i=14}^{15}
           (y_{i}-\mu)^{2}}
       {130} \\
    V(\sigma^{2} \mid y, \mu) 
      &= \frac
        {\left(\frac
          {10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
            + \sum\limits_{i=14}^{15}
            (y_{i}-\mu)^{2}}
          {20}\right)^{2}}
        {\left(\frac{15}{2} - 1\right)^{2}
          \left(\frac{15}{2} - 2\right)} \\
      &= \frac
        {3200\left(
          10\sum\limits_{i=1}^{13}(y_{i}-\mu)^2
            + \sum\limits_{i=14}^{15}
            (y_{i}-\mu)^{2}\right)}
        {1859}
  \end{aligned}
$$

***
    
(b) A biologist believes that lifetimes in days of fruitflies follow an exponential distribution. The density for a single observation y drawn from an exponential distribution is 
$$
  p(y; \lambda) = \lambda e^{-\lambda y},	\qquad y > 0.
$$
  For data, the biologist observes $100$ randomly selected fruitflies and records how many days they live.

    (i)	Write the likelihood of $\lambda$ if there are 100 data values, represented symbolically as $y_{1},\cdots,y_{100}$. What parametric family is the conjugate prior for the exponential likelihood? (Just name the family.)	[2 marks]
    
        **Answer:** For the likelihood function of an exponential distribution is:
$$
  \begin{aligned}
    L(\lambda; y_{n}) 
      &= \prod
            \limits_{i=1}^{n} 
          \lambda \, 
          e^{-\lambda\,y_{i}} \\
      &= \lambda^{n} \, 
         e^{-\lambda \sum\limits_{i=1}^{n}y_{i}}
  \end{aligned}
$$
with $n=100$ we get:
$$
  \begin{aligned}
    L(\lambda; y_{100}) 
      &= \lambda^{100} \,
         e^{-\lambda \sum\limits_{i=1}^{100}y_{i}} \\
      &= \lambda^{100} \, 
         e^{-100\lambda\bar{y}}
  \end{aligned}
$$
The cojugate prior for the exponential likelihood is Gamma distribution. 
    (ii) The biologist states that a prior distribution with mean 3 days and standard deviation 2 would reflect their previous knowledge about fruitfly lifetimes. What parameters should they specify in their prior? (numeric answer; show your work) [2 marks]
    
        **Answer:** We know that the mean and variance of $Gamma(a,b)$ are given by 
$$
  Mean = 
    \frac
      {a}{b}
    ; \quad
  Variance = 
    \frac
      {a}{b^{2}}
$$ 
So we simply need to solve the simultaneous equations 
$$
  \begin{aligned}
    \frac{a}{b} &= 3 \implies a = 3b \\
    \frac{a}{b^{2}} &=2
  \end{aligned}
$$
If we sub the value for $a$ into the second equation, we get:
$$
  \frac
    {3b}{b^{2}}
    = 2
  \implies 
    b= \frac
          {3}{2}
  \implies 
    a = \frac
          {9}{2}
$$
which leaves us with a prior of $Gamma(\frac{9}{2},\frac{3}{2})$.
  
    (iii) Based on the likelihood and the prior that you have specified, find the posterior density $p(\lambda \mid y_{1},\cdots,y_{100})$. Name it and give the numeric values of the parameters. [2 marks]
    
    **Answer:** Suppose that we have data $X_{i} \sim Exp(\lambda)$, and that the prior distribution for $\lambda$ is $Gamma(\alpha, \beta)$. Then the posterior distribution of $\lambda$ is given as
$$
  \begin{aligned}
    p(\lambda \mid \vec{x}, \alpha, \beta) 
      &\propto \lambda^{n}
        exp
          \left(
            -\lambda\sum\limits_{i=1}^{n}x_{i}
          \right) \, 
          \lambda^{\alpha-1} 
          exp
            \left(
              -\beta\lambda
            \right) \\
      &= \lambda^{n +\alpha-1}
        exp
          \left(
            -\lambda
              \left(
                \beta + \sum\limits_{i=1}^{n}x_{i}
              \right)
          \right) \\
      &= \lambda^{n +\alpha-1}
        exp
          \left(
            -\lambda
              \left(
                \beta + n\bar{x}
              \right)
          \right)      
  \end{aligned}
$$
which is equivalent to a $Gamma(\alpha + n,\beta + n\bar{x})$. Subbing in the values we determined above gives us:

$$
  \begin{aligned}
    p(\lambda \mid \vec{y}, 9/2, 3/2) 
      &\propto \lambda^{100}
        exp
          \left(
            -100\lambda\bar{y}
          \right) \, 
          \lambda^{7/2} 
          exp
            \left(
              -3/2  \lambda
            \right) \\
      &= \lambda^{100 + 7/2}
        exp
          \left(
            -\lambda
              \left(
                3/2 + 100\bar{x}
              \right)
          \right) \\
      &= \lambda^{207/2}
        exp
          \left(
            -\lambda
              \left(
                3/2 + 100\bar{x}
              \right)
          \right)      
  \end{aligned}
$$
which is equivalent to $Gamma(207/2,3/2 + 100\bar{x})$.
    

##Question 2 Total: 13 marks
The table below is adapted from the text BDA. It consists of the numbers of fatal accidents and approximate numbers of passenger miles (in 100 millions) flown each year from 1976 to 1985.

```{r Q2table, echo=FALSE}
q2.table <- matrix(
  c(24,25,31,31,22,21,26,20,16,22,3863,4300,5027,5481,5814,6033,5877,6223,7433,7107),
    ncol = 2,
    nrow = 10
  )
rownames(q2.table) <- c("1976","1977","1978","1979","1980","1981","1982","1983","1984","1985")
colnames(q2.table) <- c("Fatal Accidents", "Passenger Miles")
sum.accidents <- sum(q2.table[,1])
kable(q2.table, format="markdown", align = rep("c", dim(q2.table)[2] + 1), col.names = c("Fatal Accidents", "Passenger Miles"))
```

(a) Assume the numbers of fatal accidents are independently and identically distributed. Suggest an appropriate distribution to model the number of fatal accidents. [2 marks]

**Answer:** An appropriate model to use would be a Poisson model. We would have a parameter $\theta$ representing the *expected* number of accidents occuring in a year and data $y_{i}$ representing the *actual* number of accidents each year. So, $y_{i} \mid \theta \sim Pois(\theta)$. An appropriate distribution that we can use to model this would be a Gamma distribution:
$$y_{i} \mid \theta \sim Gamma(\alpha, \beta) = Gamma(n\bar{y_{i}}, n) = Gamma(238, 10)$$

(b) Set a suitable prior distribution for the mean parameter in (a) and determine its posterior distribution based on the data in the table above. Plot this posterior density. [3 marks]

**Answer:** From the data we can see that there have been `r sum.accidents` fatal accidents in $n = 10$ years. If we assume a non-informative $Gamma(1,0)$ prior, then the posterior distribution for the mean parameter is
  $$Gamma_{post}(\alpha, \beta) = Gamma(1 + n\bar{y_{i}}, 0 + n) = Gamma(239, 10)$$

```{r Q2bdata}

fatal.accidents <- c(24,25,31,31,22,21,26,20,16,22)
miles.flown <- c(3863,4300,5027,5481,5814,6033,5877,6223,7433,7107)
mu.y <- mean(fatal.accidents)

a0 <- 1; b0 <- 0; n <- 10

post.data1 <- rgamma(10000,a0 + n*mu.y, b0 + n)
```

```{r Q2bplot}
hist(post.data1,50)
```

(c) Find a point estimate and 95% interval for the average yearly number of fatal accidents. [2 marks]

**Answer:** We do this by going back to the Poisson model.

```{r Q2c95%CI}
theta <- rgamma(1000,n*mu.y)/(n)
theta.hat <- rpois(1000,theta)
quantile(theta.hat,c(0.025,0.975))
mean(theta.hat)
```

Here we can see an estimate of `r mean(theta.hat)` for the mean, with a $95$% CI of `r quantile(theta.hat,c(0.025,0.975))`.

(d)	Part (a) has ignored the information about passenger miles. Perhaps a better model here would take account that each fatal accident count is proportional to the number of miles flown. Set up a model of accident counts that accounts for the number of miles flown.	[2 marks]

**Answer:** Again, we have a Poisson model with an appropriate Gamma distribution. By accounting for the number of passenger miles flown ($x_{i}$), our model becomes $y_{i} \mid x_{i}, \theta \sim Pois(x_{i}\theta)$, which leads to a conjugate prior Gamma of: 
$$y_{i} \mid \theta \sim Gamma(\alpha, \beta) = Gamma(n\bar{y_{i}}, n\bar{x_{i}}) = Gamma(238, 5715.8*10^{12})$$

(e) Using this new model, set a prior distribution for the mean parameter and determine its posterior distribution. Plot this density and compare it with that in (b). [4 marks]

**Answer:** Again, assuming a non-informative conjugate prior of $Gamma(1,0)$, our posterior becomes:
$$y_{i} \mid \theta \sim Gamma_{post}(\alpha, \beta) = Gamma(1 + n\bar{y_{i}}, 0 + n\bar{x_{i}}) = Gamma(239, 5715.8*10^{12})$$

```{r Q2daccidents/mile}
mu.miles <- mean(miles.flown*10^(11))
mu.miles
a0 <- 1; b0 <- 0; n <- 10
post.data2 <- rgamma(10000,a0 + n*mu.y, b0 + n*mu.miles)
hist(post.data2,50)
qgamma(0.025,a0 + n*mu.y, b0 + n*mu.miles)
qgamma(0.975,a0 + n*mu.y, b0 + n*mu.miles)

theta <- rgamma(1000,n*mu.y)/(mu.miles*n)
theta.hat <- rpois(1000,theta*mu.miles)
quantile(theta.hat,c(0.025,0.975))
mean(theta.hat)

```
Here we can see an estimate of `r mean(theta.hat)` for the mean, with a $95$% CI of `r quantile(theta.hat,c(0.025,0.975))`. Note that these are virtually identical to the estimates arrived at by our first model. The similarities betwen the two outputs are made clearer when we plot their densities.

```{r Q2e}
plot(density(post.data1), main = "Model 1")
plot(density(post.data2), main = "Model 2")
```

Both seem to be very similar. Along with the similarities in the estimates, we should conclude that the new model has not improved our ability to estimate the true parameters of the data.

***
```{r clearQ2, echo=FALSE}
rm(list=ls())
```
##Question 3	[Total: 20 marks]

Consider the example on corn yields but with an extended data set, with sample sizes, sample means and sample variances as given below. We have five different corn growers giving individual yields for a particular new type of genetically engineered corn that has three seasons per year. The corn is distributed by a research station to the growers. Growers give yields for the most recent seasons in tons/hectare. Summary statistics for the data are given below.

```{r Q3 table, echo=FALSE}
row.symbols = c(ni='n\U1D62', ybar='y\u0305\U1D62', sigmaSq="s\U00B2\U1D62")
q3.table = matrix(c(16, 15.3, 8.2, 19, 16.2, 12.3, 14, 16.4, 7.9, 12, 13.2, 5.2, 8, 13.5, 6.2), 3, 5)
colnames(q3.table) = c("1", "2", "3","4", "5")
row.labels = c(row.symbols['ni'], row.symbols['ybar'], row.symbols['sigmaSq'])
rownames(q3.table) = row.labels
kable(q3.table, align = rep(c("c","c"), dim(q3.table)[2] + 1), caption = "Corn Yield Data by Grower", escape = TRUE)
```

(a)	By adapting code from the week 7 lecture, fit a hierarchical Normal model to this data, using the Empirical Bayes approach. Provide estimates and inference for each unknown true grower mean yield, including a plot showing all estimated posterior densities, one for each unknown mean.	[7 marks]

**Answer:** The lecture material presents us with 3 possible models, however only one is represented by the code in those same notes: the model that makes no assumptions about individual grower variances (3). So, the model we must fit to our data is a non-standard posterior distribution that is constructed by multiplying a Normal prior with the likelihood for $\mu_{j}$ in the form of a t-density.

$$
  \begin{aligned}
  p(\mu_{j} \mid y_{j}, \mu, \tau^{2}) 
  &\propto p(y_{j} \mid \mu_{j}, \mu,\tau^{2})p(\mu_{j} \mid \mu, \tau^{2}) \\
  &=p(\mu_{j} \mid \mu, \tau^{2})  \int p(\mu_{j} \mid y_{j}, \sigma_{j}^{2}, \mu, \tau^{2})(\sigma_{j}^{2} \mid \mu, \tau^{2}) \\
  &\propto p(N(\mu,\tau^{2}))p(t_{n_{j} - 1}(\bar{y}_{j}, s_{j}^{2})/n_{j}))
  \end{aligned}
$$

For the choice of hyperparameters $\mu$ and $\tau$, we again proceed in line with the notes and calculate the average yield across all growers, along with the standard deviation of that average yield. This gives us our empirical priors, which we can use to perform the estimation.
```{r Q3amanymeans}
x <- seq(5, 25, by=0.001)
n <- c(16, 19, 14, 12, 8)
y.bar <- c(15.3, 16.2, 16.4, 13.2, 13.5)
sig2 <- c(8.2, 12.3, 7.9, 5.2, 6.2)
mu <- mean(y.bar) # 14.92
tau <- sqrt(var(y.bar)) # 1.49566...
par(mfrow = c(1,2))
pdf.data <- matrix(nrow=5,ncol=20001)
for(k in 1:5){
  px <- -(x-mu)^2/(2*tau^2)-3/2*log(1+(3*(x-y.bar[k])^2)/(2*sig2[k]))
  pdf.data[k,] <- px
  if(k==1){
    plot(x,exp(px),type='l',col=k, main = "PDFs")
  } else {
    lines(x,exp(px),type='l', col = k)
  }
}
cdf.data <- matrix(nrow=5,ncol=20001)
for(k in 1:5) {
  px <- -(x-mu)^2/(2*tau^2) -3/2*log(1+(3*(x-y.bar[k])^2)/(2*sig2[k]))
  cdf.data[k,] <- exp(px)/sum(exp(px))
}

for(k in 1:5) {
  px <- cumsum(cdf.data[k,])
  if(k==1) {
    plot(x,px,type='l', col=k, main = "CDFs")
  } else {
    lines(x,px,type='l', col = k)
  }
}
```


We can generate MCMC samples using the following code (Note: I have not included the code for each grower in order to save space):

```{r MCMCex, eval=FALSE}
cdf1 <- cumsum(cdf.data[1,])
u=runif(1000,0,1)
mu1=0

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf1[k]<u[j]&cdf1[k+1]>u[j]) {
    mu1[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
    }
  }
}
```

```{r MCMC, cache = TRUE, echo = FALSE}
cdf1 <- cumsum(cdf.data[1,])
cdf2 <- cumsum(cdf.data[2,])
cdf3 <- cumsum(cdf.data[3,])
cdf4 <- cumsum(cdf.data[4,])
cdf5 <- cumsum(cdf.data[5,])

u=runif(1000,0,1)
mu1=0;mu2=0;mu3=0;mu4=0;mu5=0

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf1[k]<u[j]&cdf1[k+1]>u[j]) {
    mu1[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
    }
  }
}

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf2[k]<u[j]&cdf2[k+1]>u[j]) {
    mu2[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
  }
 }
}

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf3[k]<u[j]&cdf3[k+1]>u[j]) {
    mu3[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
  }
 }
}

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf4[k]<u[j]&cdf4[k+1]>u[j]) {
    mu4[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
  }
 }
}

for(j in 1:1000){
 st=0;k=1
 while(st==0) {
  if(cdf5[k]<u[j]&cdf5[k+1]>u[j]) {
    mu5[j] <- x[k]
    st <- 1
  } else {
    k <- k+1
  }
 }
}
```

Now, using our MCMC samples, we can calculate the posterior mean and CrI for each grower:

```{r means, echo = FALSE}
grower <- c(1,2,3,4,5)
means <- c(mean(mu1),mean(mu2),mean(mu3),mean(mu4),mean(mu5))
cil <- c(
  quantile(mu1,0.025),
  quantile(mu2,0.025),
  quantile(mu3,0.025),
  quantile(mu4,0.025),
  quantile(mu5,0.025)
  )
  
ciu <- c(
  quantile(mu1,0.975),
  quantile(mu2,0.975),
  quantile(mu3,0.975),
  quantile(mu4,0.975),
  quantile(mu5,0.975)
  )
range <- ciu-cil
int.data <- cbind(grower, means,cil,ciu, range)
kable(int.data, row.names = FALSE, col.names = c("Grower","Mean","2.5%","97.5%", "Range"))
```

This suggests an order (from the grower with the largest mean output to the lowest) of  $3,2,1,5,4$. Plotting the MCMC sample densities reaffirms this idea:

```{r MCMCplot, echo = FALSE}
plot(density(mu1),type='l',col=1, main="MCMC Densities")
text(mean(mu1),0.025, "mu1", col = 1, adj = c(-0.1, -0.1))
abline(v=mean(mu1), lty=3, col = 1)
lines(density(mu2),col=2)
text(mean(mu2),0.05, "mu2", col = 2, adj = c(-0.1, -0.1))
abline(v=mean(mu2), lty=3, col = 2)
lines(density(mu3),col=3)
text(mean(mu3),0, "mu3", col = 3, adj = c(-0.1, -0.1))
abline(v=mean(mu3), lty=3, col = 3)
lines(density(mu4),col=4)
text(mean(mu4),0, "mu4", col = 4, adj = c(-0.1, -0.1))
abline(v=mean(mu4), lty=3, col = 4)
lines(density(mu5),col=5)
text(mean(mu5),0.05, "mu5", col = 5, adj = c(-0.1, -0.1))
abline(v=mean(mu5), lty=3, col = 5)
```

(b) Plot the estimated parent density for the five grower means. At what percentile of this distribution does of each grower’s estimated posterior mean yield lie? Do they all fit in with the prior distribution? [2 marks]
```{r parentmeans, echo = FALSE}
x.parent <- seq(5,25,by=0.001)
# Grower 1
mu1.p <- round(mean(x.parent<mean(mu1))*100,2)
# Grower 2
mu2.p <- round(mean(x.parent<mean(mu2))*100,2)
# Grower 3
mu3.p <- round(mean(x.parent<mean(mu3))*100,2)
# Grower 4
mu4.p <- round(mean(x.parent<mean(mu4))*100,2)
# Grower 5
mu5.p <- round(mean(x.parent<mean(mu5))*100,2)
```
```{r Parentdata}
x.parent <- seq(5,25,by=0.001)
parent.data <- dnorm(x.parent,mu,tau)
plot(x.parent,parent.data, type = 'l', main = "Parent ~ N(mu, tau^2)")
text(mean(mu1),0.025, "mu1", col = 1, adj = c(-0.1, -0.1))
abline(v=mean(mu1), lty=3, col = 1)
text(mean(mu2),0.05, "mu2", col = 2, adj = c(-0.1, -0.1))
abline(v=mean(mu2), lty=3, col = 2)
text(mean(mu3),0, "mu3", col = 3, adj = c(-0.1, -0.1))
abline(v=mean(mu3), lty=3, col = 3)
text(mean(mu4),0, "mu4", col = 4, adj = c(-0.1, -0.1))
abline(v=mean(mu4), lty=3, col = 4)
text(mean(mu5),0.05, "mu5", col = 5, adj = c(-0.1, -0.1))
abline(v=mean(mu5), lty=3, col = 5)
```

We can easily calculate the corresponding percentile of the parent distribution for each posterior mean by:

```{r parentmeansex,eval = FALSE}
# Grower k
round(mean(x.parent<mean(muk))*100,2) # etc
```



This tells us that the posterior means for Growers 1 through 5 are: 

```{r pmeanstable, echo=FALSE}
row.n <- c("Percentile")
parent.perc <- cbind(row.n, mu1.p,mu2.p,mu3.p,mu4.p,mu5.p)
kable(parent.perc, col.names = c("Grower","1","2","3","4","5"))
```



(c) What is the posterior probability that each of the five growers has the largest mean yield? What is your conclusion in this case? [2 marks]

```{r MCMC2, echo = FALSE}
probs <- matrix(nrow = 5, ncol = 5)
n <- 1000
probs[1,1] <- length(mu1[mu1>mu1])/n # 0
probs[1,2] <- length(mu1[mu1>mu2])/n # 0
probs[1,3] <- length(mu1[mu1>mu3])/n # 0
probs[1,4] <- length(mu1[mu1>mu4])/n # 1
probs[1,5] <- length(mu1[mu1>mu5])/n # 1

probs[2,1] <- length(mu2[mu2>mu1])/n # 1
probs[2,2] <- length(mu2[mu2>mu2])/n # 0
probs[2,3] <- length(mu2[mu2>mu3])/n # 0.01
probs[2,4] <- length(mu2[mu2>mu4])/n # 1
probs[2,5] <- length(mu2[mu2>mu5])/n # 1

probs[3,1] <- length(mu3[mu3>mu1])/n # 1
probs[3,2] <- length(mu3[mu3>mu2])/n # 0.99
probs[3,3] <- length(mu3[mu3>mu3])/n # 0
probs[3,4] <- length(mu3[mu3>mu4])/n # 1
probs[3,5] <- length(mu3[mu3>mu5])/n # 1

probs[4,1] <- length(mu4[mu4>mu1])/n # 0
probs[4,2] <- length(mu4[mu4>mu2])/n # 0
probs[4,3] <- length(mu4[mu4>mu3])/n # 0
probs[4,4] <- length(mu4[mu4>mu4])/n # 0
probs[4,5] <- length(mu4[mu4>mu5])/n # 0

probs[5,1] <- length(mu5[mu5>mu1])/n # 0
probs[5,2] <- length(mu5[mu5>mu2])/n # 0
probs[5,3] <- length(mu5[mu5>mu3])/n # 0
probs[5,4] <- length(mu5[mu5>mu4])/n# 1
probs[5,5] <- length(mu5[mu5>mu5])/n # 0
```

We can calculate these probabilities fairly simply, using the MCMC estimates for the posterior means of each grower and the following code:

```{r Q3cex, eval = FALSE}
# Grower k
length(muk[muk>mua&muk>mub&muk>muc&muk>mud])/n
# where k in {1:5} and a,b,c,d in {1:5}/{k}
```

```{r Q3c, echo = FALSE}
# Grower 1
mu1.max <- length(mu1[mu1>mu2&mu1>mu3&mu1>mu4&mu1>mu5])/n
# Grower 2
mu2.max <- length(mu2[mu2>mu1&mu2>mu3&mu2>mu4&mu2>mu5])/n
# Grower 3
mu3.max <- length(mu3[mu3>mu1&mu3>mu2&mu3>mu4&mu3>mu5])/n
# Grower 4
mu4.max <- length(mu4[mu4>mu1&mu4>mu2&mu4>mu3&mu4>mu5])/n
# Grower 5
mu5.max <- length(mu5[mu5>mu1&mu5>mu2&mu5>mu3&mu5>mu4])/n

row.n <- c("Likelihood")
parent.perc <- cbind(row.n, mu1.max,mu2.max,mu3.max,mu4.max,mu5.max)
kable(parent.perc, col.names = c("Grower","1","2","3","4","5"))
```

This tells us that, using our estimates, Grower 3 has the largest mean yield with a probability of $\approx$ `r length(mu3[mu3>mu1&mu3>mu2&mu3>mu4&mu3>mu5])/n*100`%.

(d) What is the posterior probability that, for each pair of growers (grower i and i+k), the mean yield for grower(i) exceeds the mean yield for grower (i+k)? [2 marks]

**Answer:** Firstly, we can construct a matrix with all possible combinatiions of the probability statement outlined in the question. The following is example code (the full version is not reproduced to save space):

```{r Q2dex, eval = FALSE}
probs <- matrix(nrow = 5, ncol = 5)
n <- 1000
probs[1,1] <- length(mu1[mu1>mu1])/n # 0
probs[1,2] <- length(mu1[mu1>mu2])/n # 0
probs[1,3] <- length(mu1[mu1>mu3])/n # 0
probs[1,4] <- length(mu1[mu1>mu4])/n # 1
probs[1,5] <- length(mu1[mu1>mu5])/n # 1

# etc
```

Next, we can sort this data and display the results:

```{r Q3b}
# Probability that mean of Grower A (Row) is greater than mean of Grower B (Column)
colnames(probs) <- c("1","2","3","4","5")
rownames(probs) <- c("1","2","3","4","5")
probs.m <- melt(probs)

probs.m <- as.data.frame(probs.m)
names(probs.m) <- c("GrowerA", "GrowerB", "Pr")
probs.m <- probs.m %>%
  arrange(GrowerA,desc(Pr))

kable(probs.m, row.names = FALSE, col.names = c("Grower A", "Grower B", "Pr(A>B)"))
```

(e) What is the most likely ordering for the 5 mean grower yields in your MC sample? What are the 2nd, 3rd and 4th most likely orderings? [4 marks]

**Answer:** We can evaluate likely orderings by calculating all permutations of $mean(\mu_{a}>\mu_{b}\&\,\mu_{b}>\mu_{c}\&\,\mu_{c}>\mu_{d}\&\,\mu_{d}>\mu_{e}), \forall a,b,c,d,e \in {1:5}$. Doing so leaves us with:

```{r probss}
# 1st: 3>2>1>5>4
mean(mu3>mu2&mu2>mu1&mu1>mu5&mu5>mu4)
# 2nd: 2>3>1>5>4
mean(mu2>mu3&mu3>mu1&mu1>mu5&mu5>mu4)
# 3rd: 3>1>2>5>4
mean(mu3>mu1&mu1>mu2&mu2>mu5&mu5>mu4)
```
At this point we can see that the values add up to 1. Whilst other combinations are not impossible, the precision at which r has calculated these results has limited the number of possible combinations.

(f)	Based on (a)-(d), and any other evidence you can provide, where do you believe significant differences exist among the 5 grower means (if any)? [3 marks]

**Answer:** From the graph of MCMC densities, it is possible that we can discern two separate clusters of growers: Growers 4 and 5 in one; Growers 1,2, and 3 in the other. Also, all of the most likely permutations of posterior grower means only differ in the order that they list Growers 1, 2 and 3; Growers 4 and 5 are unchanged. This suggests to me that at the very least there is a significant difference between both groups. Secondly, within each group, the estimates produced by the MCMC sampling suggest very strongly that significant differences exist between each grower in the order determined to be most likely. For $3>2>1>5>4$, the probability assigned to each individual inequality (e.g. $3>2$) is $\geq 99$%. From this we should conclude that significant differences exist between - and among - the 5 growers.

###End of Assignment 2