---
title: "Lab Exercises 7 Solutions"
output:
  html_document: default
  html_notebook: default
---
```{r}
require(knitr)
opts_chunk$set(collapse = TRUE)
```

## Question 1 
In this question we will illustrate that all cdf functions really are uniformly distributed.

Firstly, simulate 5000 points from a Unif(0,1) distribution in R. (runif(k,0,1))

```{r, cache=TRUE}
x <- runif(5000,0,1)
```

#### (a) Plot the histogram of this sample. Does it look uniform?

```{r}
hist(x)
```

Yes, seems uniform

#### (b) Transform this sample using the inverse cdf function of a N(0,1) distribution $\Phi^{-1}$.

```{r}
y <- qnorm(x,0,1)
```

#### (c) What is the mean and standard deviation of your transformed set of points? Find a 95% interval for the mean and standard deviation. Do they include 0 and 1 as was expected?

```{r}
mean(y)
sd(y)
```

We can do the usual MC sample for the mean and variance of a Normal distribution

```{r}
mu <- rt(5000,4999)*sqrt(var(y)/5000)+mean(y)
sig <- 0
for( k in 1:5000) {
  a <- 5000/2 
  b <- sum((y-mu[k])^2)/2 
  sig[k] <- 1/rgamma(1,a,b)
}
```

then use

```{r}
quantile(mu,c(0.025,0.975)) 
quantile(sig,c(0.025,0.975))
```

#### (d) Plot the histogram of the transformed data. Does it look like a N(0,1)?

```{r}
hist(y)
```

Yes, as expected.

#### (e) Using the original Unif(0,1) sample, transform again, this time using the inverse cdf of a $\chi^{2}10$ distribution (qchisq)
```{r}
y <- qchisq(x,10)
```
#### (f) Plot the histogram of the transformed sample, and below plot the true density for a $\chi^{2}10$ distribution. Do they look similar? Is the estimated mean of the sample close to 10 as expected?
 
```{r}
mean(y)
var(y)
par(mfrow = c(2,1))
hist(y)
plot(density(y), lwd = 2)
``` 

## Question 2 
Consider the following 6 samples of counts of 'rejections' after heart transplant surgery from hospitals in Australia in 2000 - 2002.


```{r, echo=FALSE}
cnames <- c("Hospital","Count","Total","ML est.","Bayes flat prior est.","95%CrI","EB est.","EB95%CrI","Expert prior est.","Expert prior est.")
h1 <- c(1,1,10,0.1,"2/12=0.17","(0.023,0.41)",0.058,"(0.0035,0.181)",0.053,"(0.024,0.093)")
h2 <- c(2,0,5,0,"1/7=0.14","(0.004,0.46)",0.019,"(0,0.111)",0.048,"(0.021,0.087)")
h3 <- c(3,0,3,0,"1/5=0.20","(0.006,0.60)",0.021,"(0,0.124)",0.049,"(0.021,0.088)")
h4 <- c(4,2,35,0.057,"3/37=0.08","(0.018,0.19)",0.049,"(0.0077,0.124)",0.051,"(0.025,0.087)")
h5 <- c(5,0,12,0,"1/14=0.07","(0.0019,0.25)",0.014,"(0,0.080)",0.046,"(0.020,0.083)")
h6 <- c(6,0,17,0,"1/19=0.05","(0.0014,0.19)",0.011,"(0,0.067)",0.045,"(0.019,0.081)")
h_all <- rbind(h1,h2,h3,h4,h5,h6)
colnames(h_all) <- cnames
kable(h_all, row.names = FALSE)
```

We will fill out this table in this question and compare inferences. First enter the data in R

```{r}
y <- c(1,0,0,2,0,0)
n <- c(10,5,3,35,12,17)
mle <- y/n
```

#### (a) Find the 95% intervals for each rejection rate, treating the samples as independent and having no prior information about the rejection rates (i.e. use a *Beta(1,1)* prior).
 
The model is $y_{j}  \sim Binomial(n_j ,\theta_{j} );\>\theta_{j}  \sim Beta(1,1) \equiv Unif[0,1],\> j=1,\dots,6$. Clearly this prior is conservative as we know rejection rates are NOT 100% or even 90% and probably not even 50%, and that all rates between 0 and 1 are NOT equally likely. The posterior for each rate proportion is

$\theta_{j} \mid y_{j}	\sim Beta( y_{j} + 1, n_{j} - y_{j} + 1)$ leading to the posterior mean estimates $\hat{\theta}_j \mid y_{j}	=	\frac{y_{j} + 1}{n_j + 2}$

CrIs are obtained for hospital 1 as 

```{r}
qbeta(c(0.025,0.975),2,10)

for(k in 1:6) {
  print(qbeta(c(0.025,0.975),y[k]+1,n[k]-y[k]+1))
}
be <- (y+1)/(n+2)
```

#### (b)	Use the Empirical Bayes method to choose parameters (a,b) in a parent Beta(a,b) distribution in a hierarchical Beta-binomial model for the hospital rates. The parameters (a,b) should be chosen so that the Beta(a,b) density has the same mean and variance as that of the sample of ml estimates in the table. Using this prior, fill out the table columns headed EB. Do you see any potential problems with this prior?

The mean of the ml estimates above is $0.0262$, and the variance is $0.0018$ using 
```{r}
mean(mle)
var(mle)
```

We can find a Beta density with these characteristics by matching up the mean and variance. So, we have
$$
\frac{a}{a+b}	= 0.0262;	\frac{ab}{(a+b)^2 (a+b+1)}=0.0018	\implies a = 0.345, b = 12.829
$$
Note this follows most easily from:	
$$
\sigma^2=\frac{\mu(1-\mu)}{a+b+1},\>\text{i.e. } a + b =	\frac{\mu(1-\mu)}{\sigma^{2}}\>\text{ and }\>	a = \mu\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)\> \text{etc}			$$

This Beta(0.345, 12.829) distribution is shown below 

```{r}
x <- seq(0, 1, length=100)
px <- dbeta(x,0.345,12.829) 
plot(x,px,type='l', col="blue")
```
 
It indicates low rates are expected. (As noted, beta parameters < 1 may be more informative than intended, but we continue the analysis for demonstration purposes.)

The model is now

$$
y_{j}	\sim Binomial(n_j,\theta_{j});\>\theta_{j}\sim Beta(0.345,12.829),
$$	
leading to the posterior densities	
$$\theta_{j}	\mid y_{j}	\sim Beta(y_{j} + 0.345, n_j - y_{j} + 12.829)$$ and posterior mean estimates	

$$
\hat{\theta}_{j}	\mid y_{j}	=	\frac{y_{j} + 0.345}{n_{j} + 13.174}
$$
The CrIs in the table above are now shrunk considerably towards the prior. Six posterior densities are shown below.

```{r}
ebe <- (y+0.345)/(n+13.174)								
for(k in 1:6) {
  print(qbeta(c(0.025,0.975),y[k]+0.345,n[k]-y[k]+12.829))
}	
#Plotting EB prior and 6 posterior in the same plot
plot(x,dbeta(x,0.345,12.892),type='l',ylim=c(0,35)) 
for(k in 1:6) {
  lines(x,dbeta(x,y[k]+0.345,n[k]-y[k]+12.829),col=k+1)
}
```

Note that great care needs to be taken with this prior, due to a = 0.345. We recommend against using beta priors with any parameters less than 1 as they have undue weight near the corresponding extreme(s), which could lead to intervals that are too short when data are extreme. In these circumstances (trying to achieve a certain mean & variance pair, in the context of small proportions), use of a beta(1,a) & beta(1,b) mixture pair would avoid the issue, which is beyond the scope of this exercise.

#### (c)	You have extensive discussions with cardiac experts, nurses and staff assisting in heart transplant operations over the previous 10 years. They agree that the true rejection rate these days, with all the advances in technology, is certainly less than 10% for all hospitals in Australia. A few said the rejection rate could be as low as 1% for particular hospitals, although the consensus was that this was unlikely. Most experts said the true rate was somewhere between 3% and 7%, depending on the state of the hospital, funding, equipment and other factors. Fit a Beta(a,b) distribution to reflect this prior knowledge.

Our choice here would be to go for a prior mean of 5%, and for 99% of the distribution to be between 1% and 10%.

This means that we need	$\frac{a}{a+b}=0.05$ in our parent Beta(*a,b*) prior density.

There are so many possibilities of a and b satisfied the above equation. Starting with a Beta(1,19) and using trial and error [use R commands to check the desired approximate 99% coverage with as pbeta(0.01,a,b) and pbeta(0.1,a,b) commands], we note that a Beta(7.5,142.5) density has 99.00% of its probability between 0.01 and 0.1 and, of course, a mean of 7.5/150 = 0.05.

We will use this as the parent prior, shown below. Note that this prior is much more informative than any of the likelihood functions for each hospital, it's like observing 6.5 ex_{t}ra rejections and 141.5 ex_{t}ra non-rejections for each hospital. This prior gives us a 95% interval of (0.021, 0.090).	1

The posterior densities are now	
$\theta_{j} \mid y_{j} \sim Beta(y_{j}+7.5, n_j-y_{j}+142.5)$	and	posterior	mean	estimates	$\hat{\theta}_j \mid y_{j}=	\frac{y_{j} + 7.5}{n_j + 150}$. 

The posteriors are shown below				

##### Expert prior 

```{r}
expert <- (y+7.5)/(n+150) 
for (k in 1:6) {
  print(qbeta(c(0.025,0.975),y[k]+7.5,n[k]-y[k]+150))
}
plot(x,dbeta(x,7.5,142.5),type='l', ylim=c(0,30)) 
for(k in 1:6) {
  lines(x,dbeta(x,y[k]+7.5,n[k]-y[k]+142.5),col=k+1)
}
```

They are all now much closer together and very similar indeed to the prior, as expected, since $\alpha$ and $\beta$ are much larger in the prior than in the likelihood.

#### (d)	Fill out the remaining columns of the table above using the prior distribution for the rates your group developed in part (c).

As expected, the CrIs and estimates are all very close to each other.

#### (e) Which method is preferable, flat prior, Empirical Bayes or expert prior?

The last method appears preferable, but only if the prior information is valid. The EB approach is a useful compromise between strong prior information and NO prior information. However, the $a=0.345$ parameter is a concern.

#### (f)	How could we determine whether a significant difference existed between the hospitals? For instance, is hospital 1 significantly different to the others?

One way to quantify this would be to use MC simulation. Generate MC samples for each hospital and check the proportion of time that $\theta_1 > \theta_2$ and $\theta_1 > \theta_3$ and so on up to $\theta_1 > \theta_6$

##### Using the EB prior 

```{r}
thet1 <- rbeta(5000,y[1]+0.345,n[1]-y[1]+12.829) 
thet2 <- rbeta(5000,y[2]+0.345,n[2]-y[2]+12.829) 
thet3 <- rbeta(5000,y[3]+0.345,n[3]-y[3]+12.829) 
thet4 <- rbeta(5000,y[4]+0.345,n[4]-y[4]+12.829) 
thet5 <- rbeta(5000,y[5]+0.345,n[5]-y[5]+12.829) 
thet6 <- rbeta(5000,y[6]+0.345,n[6]-y[6]+12.829)

length(thet1[thet1>thet2]) #(OR: mean(thet1>thet2) etc)
length(thet1[thet1>thet2&thet1>thet3])
length(thet1[thet1>thet2&thet1>thet3&thet1>thet4])
length(thet1[thet1>thet2&thet1>thet3&thet1>thet4&thet1>thet5])
length(thet1[thet1>thet2&thet1>thet3&thet1>thet4&thet1>thet5&thet1>thet6])
```

So, the estimated $Pr(\theta_1 > \theta_2 \>\&\>\theta_1 > \theta_3 \>\&\>\theta_1 > \theta_4 \>\&\>\theta_1 > \theta_5 \>\&\>\theta_1 > \theta_6 ) = 0.409$ , which is reasonably high.

If we were worried about multiple testing issues we might do the following:

Assume the rates are all equal to (3/82 OR 0.1 OR 0.084, the average posterior mean). Then we could simulate from 6 binomial distributions, each with nj observations as in the table above, and determine how often we get a **maximum observed difference** of 0.1 or greater. This would account for the fact that we had chosen the maximum difference from 6 hospitals. We did this using 3/82 for 1000 replications and obtained 444 maximum observed differences above 0.1. The associated p-value would be 0.444. Clearly there is no evidence of any significant differences here among hospitals under this method. This method is quite artificial since we KNOW the rates are NOT equal to each other.

Bayesians can account for these issues by looking at all possible combinations, e.g. all possible pairs of rejection rates and looking at probabilities as a group rather than individually (as for example by taking the difference between the largest and smallest observed rates).


## Question 3
The following are rates of divorces in 8 US states, per 10,000 population, for 1995-2000.
```{r, echo=FALSE}
cnames <- c("State","Rate","Gamma(1,0) prior est.","95%CrI ","EB est.","EB 95%CrI","Expert prior est.","Expert prior 95%CrI.")
s1 <- c(1, 58, 59, "(44.9,75.0)", 61.3," (47.9,74.7)", 60.0, "(45.6,74.3)")
s2 <- c(2, 66, 67, "(51.9,84.0)", 67.5, "(53.4,81.5)",	67.3,	"(52.0, 82.4)")
s3 <- c(3, 78, 79, "(62.5,97.3)", 76.8, "(61.7,91.7)", 78.2, "(61.7, 94.5)")
s4 <- c(4, 56, 57, "(43.2,72.7)", 59.8, "(46.5,73.0)", 58.2, "(44.0, 72.3)")
s5 <- c(5, 70, 71, "(55.5,88.4)", 70.6, "(56.2,84.9)", 70.9, "(55.2, 86.5)")
s6 <- c(6, 71, 72, "(56.3,89.6)", 71.4, "(56.9,85.8)", 71.8, "(56.1, 87.5)")
s7 <- c(7, 54, 55, "(41.4,70.5)", 58.2, "(45.2,71.2)", 56.4, "(42.4, 70.2)")
s8 <- c(8, 101,	102, "(83.2,122.7)", 94.5, "(77.8,111.1)", 99.1, "(80.5, 117.5)")
s_all <- rbind(s1,s2,s3,s4,s5,s6,s7,s8)
colnames(s_all) <- cnames
kable(s_all, row.names = FALSE)
```

Rather than assuming these rates all come for the same distribution, we will build a hierarchical model for these data.

#### (a)	What model should we assume these data follow? What distribution does this suggest we use for the parameters of the model?

We would suggest a Poisson-like distribution for the data, with potentially different rate parameters for each state (given the variation in divorce laws, religious devoutness, free love, etc. among states in the US). Compared with the
 
Lecture 7 rate and exposure model, it would appear that the data we have been given is of the form $y_{j} = count j / x_{j}$ , such that this ratio is a rate per 10,000.

I.e. $y_{j} \sim$ Poisson(\theta_{j}), j=1,\cdots,8$. Recall that the Poisson likelihood is a

Gamma density in terms of the rate parameter \theta_{J}, suggesting a parent Gamma density for these rates.
```{r}
y <- c(58,66,78,56,70,71,54,101)
```

#### (b)	Proceed with a standard Bayesian analysis, assuming a diffuse prior for each state separately, and fill out the first 2 columns of the table.
 

The	diffuse	prior	considered	is the	Gamma(1,0) density, leading to $\theta_{j} \mid y_{j}  \sim Gamma( y_{j} + 1,1)$ and posterior mean estimates $\hat{\theta}_{j}\mid y_j = \frac{y_j + 1}{1} = y_j + 1$.

These densities, essentially just the likelihoods, are shown here. Perhaps significant differences exist between state 8 and states 1, 4 and 7.

We have some good data here and the CrIs are quite informative.

(Similar example code shown at (c).)

```{r}
x <- seq(0,150)
plot(x,dgamma(x,y[1]+1,1),type='l') 
for(k in 2:8) {
  lines(x,dgamma(x,y[k]+1,1),type='l',col=k)
}
for(k in 1:8){
  print(qgamma(c(0.025,0.975),y[k]+1,1))
  }
```

#### (c)	Perform a standard empirical Bayes analysis and fill out the next 2 rows of the table.
The EB analysis uses a different Gamma (a,b) prior distribution. Again we set a and b so that the average (a/b) and variance (a/b2) are equal to that observed among the 8 states divorce rates,

```{r}
mean(y)
var(y)
```

This requires that $\frac{a}{b} = 69.25; \frac{a}{b^2} = 233.4 \implies a = 20.55,b = 0.297$ We have shown this density above.
 
The prior $Gamma(20.55,0.297)$ density, leads to $\theta_{j} \mid y_{j} \sim Gamma( y_{j} + 20.55,1.297)$ and posterior mean estimates
$\hat{\theta}_{j} \mid y_{j}  = y_{j} + 20.55$. The estimates for the rates and the CrIs have not changed much at $1.297$ all, neither have the posterior densities, shown here. Much the same conclusions would be reached, comparing the states.

```{r}
b <- mean(y)/var(y)
a <- b*mean(y) 

for(k in 1:8) {
  print(qgamma(c(0.025,0.975),y[k]+a,1+b))
  }

x <- seq(0,150,.001)

px <- dgamma(x,a,b) #(prior only shown earlier)
plot(x,px,type='l',ylim=c(0,.1)) 

for(k in 1:8) {
  lines(x,dgamma(x,y[k]+a,1+b),col=k+1)
}
```

#### (d) Previous studies in the US show that the divorce rate, by state, could be anywhere between 10 and 150 per 10,000 people. Build a prior distribution and hierarchical model to reflect this information.

We would choose a prior that had, say, 99% of its probability between 10 and 150, perhaps with a prior mean at about 70, the average rate. Starting with a Gamma(70,1) we discovered that a Gamma(7, 0.1) has 99.2% of its density between 10 and 150, shown below.


We would not be surprised to see the 8 rates above come from this prior distribution.
 
#### (e) Fill in the last 2 columns of the table.

The prior Gamma(7,0.1) density, leads to  $\theta_{j} \mid y_{j}  \sim Gamma( y_{j} + 7,1.1)$ and	posterior mean estimates $\hat{\theta}_{j} \mid y_{j}	=	y_{j} +	7$. 

The estimates for the rates and the CrIs have not changed much at all, neither have the posterior densities, shown here.

Much the same conclusions would be reached, comparing the states.

The 8th state, by the way, is Nevada, which has a set of the most liberal divorce laws of any state in the US, perhaps contributing to its apparent departure from the others.
