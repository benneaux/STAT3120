---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---

# Lab Exercises 3 Solutions

## Q1
We looked at this question last week. Use Monte Carlo simulation to get the same answers as last week to 2 decimal places.

#### (a) A basketball fan watches Michael Jordan and of 100 randomly chosen free throws in 20 games, Jordan makes 89 of them. Given a uniform prior, find the posterior density for the true proportion of successful free throws and a 95% credible interval. Plot this density.

Recall that the posterior is a $Beta(90,12)$ and the answer in R is

```{r}
qbeta(0.025,90,12)
qbeta(0.975,90,12)
```

We need to simulate a $Beta(90,12)$ here. The R command $rbeta(k,a,b)$
Simulates a sample of size k from a Beta(a,b) density. Let's start with 100 MC sample

```{r}
thetaj <- rbeta(100,90,12)
quantile(thetaj,c(0.025,0.975))
```

which returns (for example: remember, this is a random sample) $(0.819, 0.938)$, and so even 100 MC samples gives 1-2 decimal places of accuracy. Let's try 1000 samples:

```{r}
thetaj <- rbeta(1000,90,12)
quantile(thetaj,c(0.025,0.975))
```

which gives $(0.8200480, 0.9327415)$ again 1-2 decimal accuracy. Now 10000 samples

```{r}
thetaj <- rbeta(10000,90,12)
quantile(thetaj,c(0.025,0.975))
```

which gives $(0.8144603, 0.9369110)$ and we have 3 decimal places of accuracy. We can plot the histogram of our sample to approximate the true density:

```{r}
hist(thetaj,50)
```

where 50 is the number of bines in the histogram hist(theta) uses a default number of bins.

R has a density estimator/smoother:

```{r}
plot(density(thetaj))
```

The plot below shows a $beta(90,12)$ density overlaid onto the density estimate above from 100000 MC samples. The commands are

```{r}
plot(density(thetaj),xlim=c(0.7,0.97))
theta1 <- seq(0,1,.001)
lines(theta1,dbeta(theta1,90,12),type='l',lty=2)
```

 
#### (b) The same fan watches Shaquille O'Neal make only 40 out of 100 randomly chosen free throws from 20 games. Find the posterior density for his success proportion using a uniform prior and a 95% credible interval. Plot this density.

Recall that $\theta_{O} \mid y_{O} \sim Beta( 1, 61)$ and a 95% CrI was $(0.309, 0.498)$. So,

```{r}
thetao <- rbeta(100,41,61)
quantile(thetao,c(0.025,0.975))
```

gives $(0.3280602, 0.4925870)$ and we have 1-2 place accuracy.

```{r}
thetao <- rbeta(10000,41,61)
quantile(thetao,c(0.025,0.975))
```

gives $(0.3088187, 0.4962563)$ and we have 2 decimal places of accuracy.

```{r}
thetao <- rbeta(100000,41,61)
quantile(thetao,c(0.025,0.975))
```

gives $(0.3093610, 0.4979226)$ and 3 places of accuracy.


#### (c)	Use what you know about these two players (if anything) to construct a Beta prior distribution for their success proportions at making free-throws and repeat the analysis in (a) and (b).

We refer you to Lab 2 Solutions: Suppose you suspect that Jordan's percentage is somewhere between 0.75 and 0.95. Based on a prior mean of 0.85 and prior variance of $0.0025 = [(0.95-0.75)/4]^2$, we derived $a = 42.5 and b.= 7.5$.

Now, $Y_{j} \sim Bin(n =100, \theta_{J})$	and $\theta_{J} \sim Beta(42.5, 7.5)$, $p(\theta_{J}  \mid Y_{j} ) \propto p( Y_{j}  \mid \theta_{J} ) p(\theta_{J} ) \propto \theta_{J}89 (1' \theta_{J} )11\theta_{J}41.5 (1' \theta_{J} )6.5 = \theta_{J}130.5 (1' \theta_{J} )17.5$

So, $\theta_{J}  \mid Y_{j} \sim Beta( 131.5, 18.5)$.

The mode estimate is now $130.5/148 = 0.882$; posterior mean is $131.5/150 = 0.877$ and a 95% posterior CrI is $qbeta(0.025,131.5,18.5) = 0.820$ and $qbeta(0.975,131.5,18.5) = 0.924$.

Re-doing it with 10000 MC samples

```{r}
thetaj <- rbeta(10000,131.5,18.5)
quantile(thetaj,c(0.025,0.975))
```

which gives $(0.8200350, 0.9244421)$ We refer you to Lab 2 Solutions.

Now suppose that you suspect that O'Neal's percentage is very close to 50% and very likely between 40 and 60%. We choose a prior mean of 0.5 with prior variance of $[(0.6-0.4)/4]^2 = 0.0025$.

This means that (from before):

$$\begin{aligned}
Y_{O} &\sim Bin(n =100, \theta_{O})\\ 
\theta_{O} &\sim Beta(49.5, 49.5) \\
p(\theta_{O} \mid yO ) &\propto p( yO \mid \theta_{O} ) p(\theta_{O} ) \\
&\propto \theta_{O}^{40} (1-\theta_{O})^{60} \theta_{O}^{48.5} (1-\theta_{O})^{48.5} \\
&= \theta_{O}^{88.5} (1- \theta_{O} )^{108.5}
\end{aligned}
$$
So, $\theta_{O} \mid y_{O} \sim Beta(89.5, 109.5)$.
 
The mode estimate is now $88.5/197 = 0.449$; posterior mean is $89.5/199 = 0.450$ and a 95% posterior CrI is $qbeta(0.025,240.5,260.5) = $ `r qbeta(0.025,240.5,260.5)` & $qbeta(0.975,240.5,260.5) = $`r qbeta(0.975,240.5,260.5)`.

Re-doing it with 10000 MC samples

```{r}
thetao <- rbeta(10000,89.5,109.5)
quantile(thetao,c(0.025,0.975))
```

which gives $(0.3800281, 0.5193504)$

#### (d) What is the probability that Jordan's free-throw percentage is above 90% from part (a)? from part (c)?

Instead of the Lab 2 Solutions, Question 3 (d), e.g. 

```{r}
1-pbeta(0.9,90,12)
```

given our vector $\theta$ we can approximate this by

```{r}
sum(thetaj>0.9)/10000 # (or: mean(theta>0.9))
```

#### (e) How many iterations did you need to get 2 decimal places of accuracy? What about 3 or more? How many places of accuracy do we need?

It took 100 iterations to get 1-2 places and up to 1000000 to get 3 places of accuracy. In this problem, 2 decimal places is clearly enough accuracy to make decisions about a proportion which is between 0 and 1.

#### (f) How can we compare the two success proportions? Is Jordan significantly better than O'Neal? What is the probability that Jordan is better than O'Neal? Use MC simulation to answer.

We just need to estimate $Pr(\theta_{J} > \theta_{O})$ . In the samples of size 100000, all 100000 iterates had $\theta_{J} > \theta_{O}$, i.e.

```{r}
length(thetaj[thetaj>thetao])	# (or: mean(thetaj>theta0))
```

returned 100000, so our estimate is $Pr(\theta_{J} > \theta_{O}) \sim = 1$.

#### (g) More interestingly, do this analysis for if O'Neal's performance had been 80 out of 100. Provide an appropriate graph.

E.g.

```{r}
thetaj <- rbeta(1000000,90,12)
quantile(thetaj,c(0.025,0.975))
thetao <- rbeta(1000000,81,21)
quantile(thetao,c(0.025,0.975))
```

I.e., substantial overlap (see graph too)

```{r}
mean(thetaj>thetao)
```

I.e., we still have 'statistical significance'!

```{r}
theta <- seq(0,1,.001)
plot(theta,dbeta(theta,90,12),type='l')
lines(theta,dbeta(theta,81,21),type='l')
```

## Q2 
Consider again the placenta previa example, where y = no. of female births and $\theta	= Pr(female birth)$; we will take a sample of $n=1000$.

#### (a)	After consultation with a specialist in Placenta previa, you place a $Beta(100,100)$ prior on \theta. Show that this means that you (and the doctor) are more than 95% sure that $0.4 < \theta < 0.6$; but that you are undecided whether $\theta > 0.5$ OR $\theta < 0.5$.

If we have $\theta \sim Beta(100,100)$ we can easily answer questions such as $Pr(0.4 < \theta < 0.6) = pbeta(0.6,100,100) - pbeta(0.4,100,100) = 0.9956798 > 0.95$
And
$Pr(\theta < 0.5) = pbeta(0.5,100,100) = 0.5$
thus verifying the two statements in the question. We could also do this via MC simulation:

```{r}
theta <- rbeta(10000,100,100) 
length(theta[theta<0.5])/10000 #(or: mean...) 
(length(theta[theta<0.6])-length(theta[theta<0.4]))/10000
```

Which gives $0.5032$ and $0.9949$ which are correct to 2-3 decimal places.

#### (b) Is this a reasonable prior distribution?

Good question. It represents an expert's opinion and puts almost all probability between 0.4 and 0.6. It is pictured below

```{r}
x1 <- seq(0.01,1,0.01)
px <- dbeta(x1,100,100)
plot(x1,px,type = 'l', xlim=c(0.3,0.7))
```

#### (c)	(Masters).
What does this prior imply about the possible observations, y, before we observe any data? Does this seem reasonable?

Recall the prior predictive density is a beta-binomial							
$$p(y_1) =	\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(n + 1)}{\Gamma(y_1	+ 1)(n - y_1 + 1)}\frac{\Gamma(y_1 + a)\Gamma(n-y_1 + b)}{\Gamma(a+b+n)},\qquad y_1 = 0,\dots,
n$$
For a=b=100 we have

$$
p(y_1) =	\frac{\Gamma(200)}{\Gamma(100)\Gamma(100)}\frac{\Gamma(n + 1)}{\Gamma(y_1	+ 1)(n - y_1 + 1)}\frac{\Gamma(y_1 + 100)\Gamma(n-y_1 + 100)}{\Gamma(100+100+n)},\qquad y_1 = 0,\dots,n
$$

Let's consider $n=10, 100, 1000, 10000$
 
```{r}
bb2 <- function(y,n,a,b) {
  g1 <- lgamma(a+b)-(lgamma(a)+lgamma(b))
  g2 <- lgamma(n+1)-(lgamma(y+1)+lgamma(n-y+1))
  g3 <- (lgamma(y+a)+lgamma(n-y+b))-lgamma(a+b+n)
  g <- g1+g2+g3
  exp(g)
}
```

```{r}

n <- c(10,100,1000,10000)
a <- 100
b <- 100

par(mfrow=c(2,2))

for(i in 1:length(n)){
  y <- seq(0,n[i],1)
  py <- vector(mode='numeric', length = n[i])
  for(k in 1:length(y)){
    py[k] <- bb2(y[k],n[i],a,b)
  }
    plot(y, py, type = 'l', main = paste("n = ",n[i]))
}
```


The prior predictive puts most probability on y for y/n = 0.4 to 0.6 as would be expected from the Beta(100,100) prior which does the equivalent thing.

##### (d)	The sample of $n=1000$ is taken and 511 males are born. Update your opinion about whether $\theta > 0.5$ OR $\theta < 0.5$.

$\theta	\mid y=489, n=1000 \sim Beta(489+100, 511+100) = Beta(589, 611), Pr(\theta >0.5 \mid y=489, n=1000) = 1 - pbeta(0.5,589,611) = 0.263$. The density is plotted below with prior on top and posterior on bottom.
```{r}
par(mfrow=c(2,1))
x <- seq(0.001,1,0.001)
plot(x,dbeta(x,100,100),type='l')
plot(x,dbeta(x,589,611),type='l')
```

##### (e) Repeat (d) under a (i) Beta(1,1) prior (ii)	Beta(a,b) prior where a and b are chosen so that the prior mean = 0.5 and prior standard deviation = 0.1

###### (i)	
Under a $Beta(1,1)$ prior, $\theta \mid y=489, n=1000 \sim Beta(489+1, 511+1) = Beta(490, 512), Pr(\theta >0.5 \mid y=489, n=1000) = 1 - pbeta(0.5,490,512) = 0.243$

###### (ii)	
$\frac{a}{a+b} = 0.5 \implies a = b$	and	$\frac{a + b}{(a + b)^2 (a + b + 1)}=\frac{a^2}{4a^2(2a+1)}=\frac{1}{8a+4}=0.1^2\implies a = b = 12$
Under a $Beta(12,12)$ prior prior, $\theta \mid y=489, n=1000 \sim Beta(489+12, 511+12) = Beta(501, 523), Pr(\theta >0.5 \mid y=489, n=1000) = 1 - pbeta(0.5,501,523) = 0.246$

#### (f) How sensitive are the results to choice of prior?

The results seem not sensitive to prior choice, especially for priors with mean close to 0.5.

## Q3 Adapted from BDA, Section 1.12, question 1.
Let $y \sim N (\theta ,4)$ , so that $y$ is Normally distributed with unknown mean $\theta$ and known variance $4$. Let the prior distribution on the mean $\theta$ be

$$
\theta \quad 1 \quad 2 \\
Pr(\theta)\quad 0.5\quad 0.5
$$

Thus $\theta$ can only take the values $1$ or $2$, both with equal probability.

#### (a) A single sample of $y=1$ is observed. Update the discrete probability distribution for $\theta$ using Bayes' rule. i.e. find $p(\theta \mid y = 1)$.
$$
p(\theta \mid y = 1)	\propto p( y = 1\mid \theta ) p(\theta ) = (8\pi )^{-0.5}exp\left(-\frac{1}{8}(y-\theta )^2\right)p(\theta )	\\
=dnorm(1,\theta,\sqrt4)/2
$$

for	$\theta = 1$ and then $\theta = 2$
So the posterior is $Pr ( \theta =1 \mid y=1) = 0.531, Pr (\theta =2 \mid y=1) = 0.469$.

#### (b) A single observation of $y=1.5$ is observed. Update the prior, ignoring the data point in part (a).

$p(\theta \mid y = 1.5) \propto p(y = 1.5 \mid \theta)p(\theta) = (8\pi)^{-0.5}exp\left(-\frac{1}{8}(1.5-\theta)^2\right)p(\theta)$

```{r, echo = FALSE}
names <- c("theta",	"Prior", "Likelihood","Product","Posterior")				
one <- c(1,	0.5,	0.1933,		0.09665	,0.5	)					
two <- c(	2	,0.5,	0.1933,		0.09665,	0.5	)					
sum <- c("","",	"Sum",	0.1933,"")

data <- rbind(one,two,sum)
knitr::kable(data,row.names = FALSE,col.names = names)
```

$=dnorm(1.5,\theta,sqrt(4))*0.5$	for  $\theta = 1$ and then $\theta = 2$						
So the posterior is $Pr (\theta =1 \mid y=1.5) = 0.5,Pr ( \theta =2 \mid y=1.5) = 0.5$.

Clearly this point doesn't give us ANY extra info about $\theta$ since it is exactly half way between 1 and 2!

#### (c) Update the prior using Bayes' rule after observing both sample points in (a) and (b). That is we have a sample of size $2, y1 = 1; y2 = 1.5.$

$$
p(\theta \mid y = 1,1.5)	\propto p( y = (1,1.5) \mid \theta ) p(\theta ) = (8\pi)^{-1}exp\left(-\frac{1}{8}\left[(1	- \theta)^2 +(1.5	- \theta)^2\right]\right)p(\theta )	
$$
```{r, echo = FALSE}
names <- c("theta",	"Prior", "Likelihood","Product","Posterior")				
one <- c(1,	0.5,	0.0386,		0.0193	,0.531	)					
two <- c(	2	,0.5,	0.0340,		0.017,	0.469	)					
sum <- c("","",	"Sum",	0.0363,"")

data <- rbind(one,two,sum)
knitr::kable(data,row.names = FALSE,col.names = names)
```

$=dnorm(1,\theta,sqrt(4))* dnorm(1.5,\theta,sqrt(4))*0.5$  for	$\theta = 1$ and then $\theta = 2$

So the posterior is $Pr ( \theta =1 \mid y=1) = 0.531,Pr ( \theta =2 \mid y=1) = 0.469$.
This is the same as in (a). The point 1.5 is exactly between 1 and 2 and gives NO information either way about $\theta =1$ or $2$, as shown in (b).

#### (d) Use probability rules to find the marginal likelihood p(y), i.e. the likelihood unconditional on $\theta$. Plot this likelihood over a sensible grid of points.

Use probability rules to find the marginal likelihood p(y), i.e. the likelihood unconditional on $\theta$. Plot this likelihood over a sensible grid of points.
$$p( y) = \sum p(y \mid \theta ) p(\theta ) = \sum(8\pi)^{-0.5}exp\left(-\frac{1}{8}(y	- \theta)^2 \right)p(\theta ) \\
=0.5(8\pi)^{-0.5}\left[exp\left(-\frac{1}{8}(y	- 1)^2\right) +exp\left(-\frac{1}{8}(y-2)^2\right)\right]
$$															
e.g.

```{r}
y <- seq(-15,10,.001)
py <- (dnorm(y,1,sqrt(4))+dnorm(y,2,sqrt(4)))*0.5
plot(y,py,type='l')
```

####(e) What is the mean and variance of $p(y)$?

$$\begin{aligned}
E(Y) &= E[E(Y \mid \theta )] \\
&= E(\theta ) \\
&= 1* 0.5 + 2 * 0.5 \\
&= 1.5 \\
Var(Y) &= Var[E(Y \mid \theta )] + E[Var(Y \mid \theta )] \\ &= Var(\theta ) + E[4] \\
&=	E(\theta^2 ) - E(\theta)^2 + 4 \\
&=	12 * 0.5 + 22 * 0.5 - 1.5^2 + 4 \\
&=	4.25
\end{aligned}
$$

You could use integration here but it gets messy!

#### (f) Can you simulate from $p(y)$? If so, simulate a sample of size $10000$ and check the sample mean and variance are close to the theoretical values in (b). Estimate a 95% CrI for y based on the sample.

To simulate note that 50% of values (on average) should come from the N(1,4) and 50% on average from the N(2,4). So first simulate 10000 uniform numbers in (0,1). Each time the uniform number > 0.5
simulate from the N(1,4) otherwise simulate from N(2,4). A histogram of this sample is

The R code is

```{r}
y <- 0
for(k in 1:10000) {
  if(runif(1,0,1)>0.5) {
    y[k]=rnorm(1,2,sqrt(4))
    } else {
      y[k] <- rnorm(1,1,sqrt(4))
    }
}
hist(y)
```

The mean and variance of my sample is $1.51$ and $4.24$ respectively as expected. A 95% CrI for y is $quantile(y,c(0.025,0.975)) = $`r quantile(y,c(0.025,0.975))`.

This is an example of a mixture of 2 Normal distributions.


