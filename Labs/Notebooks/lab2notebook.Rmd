---
title: "Lab Exercises 2 Solutions"
output:
  html_document: default
  html_notebook: default
---
## Question 1 
Albert, 1997, Chapter 3.4, Exercise 5: A geneticist is investigating the link between two fruit-fly genes, and plans a test cross. He has only two hypotheses for genes of the fruit-fly offspring. Under hypothesis A and B, the proportions of offspring of each type are in the table below:
```{r}

c.names	<- c("Hypothesis","Type 1",	"Type 2",	"Type 3",	"Type 4")
A <- c("A", 0.5625,	0.1875,	0.1875,	0.0625)
B	<- c("B",0.25,	0.25,	0.25,	0.25)
data <- rbind(A,B)
colnames(data) <- c.names
knitr::kable(data, row.names = FALSE)
```

The fruit flies were mated and the following numbers of offspring of each type were observed:
```{r}
vals <-c(8,2,4,3)
#knitr::kable(vals, row.names = FALSE,col.names = c("Type 1","Type 2","Type 3","Type 4"))
```

##### (i)	What prior could we use for each hypothesis?

In this situation, rather than estimation and inference, the geneticist is more interested in which hypothesis is supported by the data.In this question we have two competing hypotheses, and some observed counts of data, y. 

We do NOT know the prior probability of each hypothesis, we may assume Pr(A) = Pr(B) OR some other probabilities based on previous work, or consultation with genetic theory or geneticists. It is often safer to assume Pr(A) = Pr(B).

##### (ii) Which of the two hypotheses is more likely based on the data?

In this situation, rather than estimation and inference, the geneticist is more interested in which hypothesis is supported by the data.In this question we have two competing hypotheses, and some observed counts of data, y. 

A Bayesian will estimate Pr(A \mid y) and Pr(B \mid y) (as if they are the only two possible outcomes) and use these quantities to determine the most likely hypothesis.

This leads to	
$$Pr(A \mid y) = \frac{Pr(y \mid A)Pr(A)}{Pr(y \mid A)Pr(A) + Pr(y \mid B)Pr(B)} = \frac{1}{1 + \frac{Pr(y\mid B)Pr(B)}{Pr(y\mid A) Pr(A)}}$$

The ratio of likelihoods in the denominator is called the Bayes factor (which has a different interpretation when hypotheses are composite). If the Bayes factor is greater than 1, and $Pr(A) = Pr(B)$, then hypothesis A is more likely than hypothesis B, while if it is less than 1 then hypothesis B is more likely.

For the likelihoods, we use a multinomial model with 4 categories (a binomial has 2 possible categories each trial, a multinomial has more than 2). The likelihood for a multinomial with $y_1, y_2, y_3$ and $y_4$ counts in each category is

$$
Pr(y \mid \theta) = \frac{n!}{y_1!y_2!y_3!y_4!}\theta_1^{y_1}	\theta_2^{y_2} \theta_3^{y_3}(1- \theta_1- \theta_2- \theta_3 )^{y_{4}} \propto \theta_1^{y_1}	\theta_2^{y_2} \theta_3^{y_3}(1- \theta_1	- \theta_2 - \theta_3)^{y_4}
$$

ignoring the integration constant. Thus the likelihood under each hypothesis is:

$$
Pr(y \mid A) \propto 0.5625^8 0.1875^2 0.1875^4 0.0625^3 = 1.06 \times 10^{-10} \\
Pr(y \mid B) \propto 0.25^8 0.25^2 0.25^4 0.25^3 = 5.82 \times 10^{-11}
$$

Note that we have ignored the combinatorial term in front of the likelihood as it is the same (i.e., constant) for each hypothesis (it depends on y only).

By inspection of the data, hypothesis A appears more likely than B.

As mentioned above, here the Bayes factor is just the ratio of likelihoods

$$\frac{p(y \mid A)}{p(y \mid B)} = 1.06 \times 10^{-10} /(5.82 \times 10^{-11}) = 1.83$$

In other words, based on the data, hypothesis A is 1.83 times as likely as hypothesis B. We find that

$$Pr(A \mid y) = 1/(1+1/1.83) = 0.647$$,

and see that A is indeed the more likely hypothesis based on the data.

## Question 2 
Albert, Ch. 4, Exercise 4.4, no. 4. A woman tells you she can predict the sex of a baby by how high it 'rides' in the mother's uterus. 'High' means a boy and 'low' means a girl. You suspect she is only guessing, but you are not completely sure;

#### (a) Place a prior distribution on the values of her success rate (p) in the table below that reflects this belief that she is probably just guessing.

Success rate (p)	0	0.25	0.5	0.75	0.9	1
Pr(p) (prior)	0	0.1	0.75	0.1	0.04	0.01

This is just one example. We give 0 prior to p=0 as even if she is just guessing she will have a positive success rate. The smallest positive probability is on p=1, just in case her theory is accurate. The prior is plot below.

Use commands 

```{r}
p <- c(0,0.25,0.5,0.75,0.9,1)
prior <- c(0,0.1,0.75,0.1,0.04,0.01)
plot(p,prior,type='l')
```

(if you cut and paste these lines and get an error, you might need to re-type the plot line, as Word uses its own quotation marks)

#### (b)	You conduct an experiment with 10 pregnant women, and the woman is correct 7 times out of 10 in predicting the sex of the unborn baby. Update your probabilities in the table.

```{r, echo = FALSE}
srate <- c(0,0.25,	0.5,	0.75,	0.9,	1)
prior <- c(0,	0.1,0.75,	0.1,	0.04,	0.01)
like <- c(0,	0.00309,	0.117,	0.25,	0.0574,	0)
prod <- c(0,	0.00031,	0.0878,	0.025,	0.00230,	0)
post <- c(0,	0.00267,	0.761,	0.217,0.0199,	0)
data <- cbind(srate,prior,like,prod,post)
names_data <- c("Success rate (p)","Pr(p) (prior)", "Likelihood","Product",	"Posterior")
knitr::kable(data,row.names = FALSE, col.names = names_data)
```

The binomial model is assumed here, so

$$
Pr(y=7\mid p) = \binom{10}{7}p^7(1-p)^3			
$$	

which was used in the Likelihood row above.

Use the function 

```{r}
like <- dbinom(7,10,p)
```

in R where p is each possible value for p to evaluate the likelihood function.

Then

```{r}
posterior <- like*prior
posterior <- posterior/sum(posterior)
```

The likelihood reflects that the data support p = 0.75 the most, while the posterior still favours that she is guessing. The plot below shows prior, likelihood and posterior. To get three plots
on one window type 

```{r}
par(mfrow=c(3,1))
```

in R. Then 

```{r}
plot(p,prior,type='l')
plot(p,like,type='l')
plot(p,posterior,type='l')
```

(again, you might need to replace the quotation marks if cutting and pasting)

#### (c)	How likely is she to have some skill in gender prediction? How likely is she to be just guessing?

We could interpret skill as p>0.5.

$$
Pr( skill \mid y=7) = Pr(p>0.5 \mid y = 7) = 0.217+0.02 = 0.237. \\
Pr (guessing \mid y=7) = Pr( p =0.5\mid y=7) = 0.761
$$

The posterior still favours that she is guessing.

#### (d) Repeat (b) and (c) with a flat prior on p.
```{r}
srate <- c(0,0.25,	0.5,	0.75,	0.9,	1)
prior <- c(0.167,	0.167,0.167,	0.167,	0.167,	0.167)
like <- c(0,	0.00309,	0.117,	0.25,	0.0574,	0)
prod <- c(0,	0.000515,	0.0195,	0.0417,	0.00957,	0)
post <- c(0,	0.0072,	0.274,	0.585,0.134,	0)
data <- cbind(srate,prior,like,prod,post)
names_data <- c("Success rate (p)","Pr(p) (prior)", "Likelihood","Product",	"Posterior")
knitr::kable(data,row.names = FALSE, col.names = names_data)
```
$$
Pr(skill \mid y=7) = Pr(p>0.5 \mid y = 7) = 0.719. \\

Pr (guessing \mid y=7) = Pr( p =0.5\mid y=7) = 0.274
$$

The posterior (likelihood) now favours that she has some skill. Think about how much evidence you would REALLY need to be convinced that she has some skill in predicting sex. Is 7 out of 10 enough? If we really believed in a flat prior then maybe it is, but remember that the sample size is only 10.

## Question 3

#### (a) A basketball fan watches Michael Jordan and randomly chooses 100 of his free throws in 20 games, Jordan makes 89 of them. Given a uniform prior, find the posterior density for the true proportion of successful free throws and a 95% credible interval. Plot this density.

If we let Y_{j} = number of successful free throws for Jordan then we can assume Y_{j} \sim Bin(n =100, \theta_{J}) and \theta_{J} \sim Unif(0,1) = Beta(1,1)

p(\theta_{J} \mid Y_{j} ) \propto p( Y_{j} \mid \theta_{J} ) p(\theta_{J} ) \propto \theta_{J}89 (1' \theta_{J} )11\theta_{J}1'1 (1' \theta_{J} )1'1 = \theta_{J}90'1 (1' \theta_{J} )12'1 So, \theta_{J} \mid Y_{j} \sim Beta( 90, 12). We can plot this on a grid of points as follows in R:

```{r}
x <- (0:1000)/1000 # (easier: seq(0,1,.0001)) 
px <- dbeta(x,90,12)

plot(x,px,type='l')
title("Jordan free throws, Beta(90,12)")
```

The density is peaked at 0.89 as expected, the posterior mean estimate is 90/102 = 0.88. Use

```{r}
qbeta(0.025,90,12)
```

and

```{r}
qbeta(0.975,90,12)
```

to generate the 95% credible interval for the $Beta(90,12)$ distribution.

#### (b) The same fan watches Shaquille O'Neal make only 40 out of 100 randomly chosen free throws from 20 games. Find the posterior density for their success proportion using a uniform prior and a 95% credible interval. Plot this density.

If we let Y_{O} = number of successful free throws for O'Neal then we can assume Y_{O} \sim Bin(n =100, \theta_{O}) and \theta_{O} \sim Unif(0,1) = Beta(1,1)

p(\theta_{O} \mid yO ) \propto p( yO \mid \theta_{O} ) p(\theta_{O} ) \propto \theta_{O}40 (1' \theta_{O} )60 \theta_{O}1'1 (1 ' \theta_{O} )1'1 = \theta_{O}41'1 (1 ' \theta_{O} )61'1 So, \theta_{O} \mid yO \sim Beta( 41, 61). We can plot this on a grid of points as follows in R:

```{r}
x<-(0:1000)/1000 
px<-dbeta(x,41,61) 
plot(x,px,type='l')

title("O'Neal free throws, Beta(41,61)")
```


The density is peaked at $0.4$, with posterior mean $41/102 = 0.402$.

Use $qbeta(0.025,41,61) =$ `r qbeta(0.025,41,61)` and $qbeta(0.975,41,61) =$ `r qbeta(0.975,41,61)` in R to generate the 95% (central) credible interval for the $Beta(41,61)$ distribution.

#### (c) Use what you know about these two players (if anything) to construct a Beta prior distribution for their success proportions at making free-throws and repeat the analysis in (a)-(b).

Suppose you suspect that Jordan's percentage is somewhere between $0.75$ and $0.95$. Choose a prior mean of $0.85$ with prior variance of $0.0025 = [(0.95-0.75)/4]^2$

This means that:
$$
	\frac{a}{a + b}	= 0.85;	\> \frac{ab}{(a + b)^{2}(a + b + 1)} = 0.0025 \\	
b =	\frac{3a}{17}\implies a = 42.5,\>b = 7.5 \> (*)		
$$

(For general purposes, it's probably easier to write $\mu$ and $\sigma^{2}$ as functions of a and b.) Now, $Y_{j} \sim Bin(n =100, \theta_{J})$ and $\theta_{J} \sim Beta(42.5, 7.5), p( \theta_{J} \mid Y_{j} ) \propto p(Y_{j} \mid \theta_{J})p(\theta_{J}) \propto \theta_{J}^{89}(1- \theta_{J})^{11}\theta_{J}^{41.5}(1- \theta_{J})^{6.5} = \theta_{J}^{130.5}(1- \theta_{J})^{17.5}$

So, $\theta_{J} \mid Y_{j} \sim Beta( 131.5, 18.5)$. The prior, likelihood and posterior are plotted below. The model estimate is now $130.5/148 = 0.882$; posterior mean is $131.5/150 = 0.877$ and a 95% posterior CrI is $qbeta(0.025,131.5,18.5) =$ `r qbeta(0.025,131.5,18.5)` and $qbeta(0.975,131.5,18.5) = $ `r qbeta(0.975,131.5,18.5)`.

(*) A more general method is based on recognising that

$$mu =	\frac{a}{a+b},	\sigma^{2}	=	\frac{ab}{(a + b)^2 (a + b + 1)}	\implies \sigma^{2} =	\frac{\mu(1 - \mu)}{(a + b + 1)	}$$
i.e.
$$

a + b =	\frac{\mu(1-\mu)}{\sigma^2}-1 \text{ and } a = \mu(a + b) = \mu\left[\frac{\mu(1- \mu)}{\sigma^2}-1\right]; \>	b =	(1-\mu)\left[\frac{\mu(1-\mu)}{\sigma^2}-1\right]
$$

```{r}
par(mfrow=c(3,1))
x<-(0:1000)/1000 
px <- dbeta(x,42.5,7.5)
pl<-dbeta(x,90,12) 
pp <- dbeta(x,131.5, 18.5)

plot(x,px,type='l') 
plot(x,pl,type='l') 
plot(x,pp,type='l')
```
The posterior distribution is slightly narrower than that for the Uniform prior for Jordan's unknown free throw rate. The above analysis is an example of a specific prior only.

Now suppose that you suspect that O'Neal's percentage is very close to 50% and certainly between $40$ and 60%. We choose a prior mean of 0.5 with prior variance of $0.0025 = [(0.6-0.4)/4]^2$

This means that:

$$
	\frac{a}{a + b}	= 0.5;	\> \frac{ab}{(a + b)^{2}(a + b + 1)} = 0.0025 \\	
b =	a\implies a = 49.5,\>b = 49.5		
$$		
Now, $Y_{O} \sim Bin(n =100, \theta_{O})$	and $\theta_{O} \sim Beta(49.5, 49.5), p(\theta_{O} \mid yO ) \propto p( yO \mid \theta_{O} ) p(\theta_{O} ) \propto \theta_{O}40 (1-\theta_{O} )^{60} \theta_{O}^{48.5} (1 - \theta_{O} )^{48.5} = \theta_{O}^{88.5} (1- \theta_{O} )^{108.5}$

So, $\theta_{O} \mid yO  \sim Beta(89.5, 109.5)$. The prior, likelihood and posterior are plotted below.

The model estimate is now $88.5/197 = 0.449$; posterior mean is $89.5/199 = 0.450$ and a 95% posterior CrI is $qbeta(0.025,89.5,109.5) = $ `r qbeta(0.025,89.5,109.5)` & $qbeta(0.975, 89.5,109.5) = $ `r qbeta(0.975, 89.5,109.5)`.

```{r}
par(mfrow=c(3,1))
x<-(0:1000)/1000 
px <- dbeta(x,49.5,49.5)
pl<-dbeta(x,41,61) 
pp <- dbeta(x,89.5, 109.5)

plot(x,px,type='l') 
plot(x,pl,type='l') 
plot(x,pp,type='l')
```

#### (d) What is the probability that Jordan's free-throw percentage is above 90% from part (a)? from part (c)?

$$
Pr(\theta_{J} >0.9\mid \theta_{J} \sim Beta(90,12)) = 1-pbeta(0.9,90,12) = 0.309 \\
Pr(\theta_{J} >0.9\mid \theta_{J} \sim Beta(131.5,18.5)) = 1-pbeta(0.9,131.5,18.5) = 0.196
$$
We are not sure in either case; 0.9 seems close to the centre of the possible values for $\theta_{J}$.

#### (e) How can we compare the two success proportions? Is Jordan significantly better than O'Neal? What is the probability that Jordan is better than O'Neal?

We can compare the 2 posterior distributions. Assuming the uniform prior, $\theta_{J} \sim Beta(90,12)$ and $\theta_{O} \sim Beta(41,61)$. We might wish to know $Pr(\theta_{J} > \theta_{O} \mid Y_{j}, yO) = Pr(\theta_{J} - \theta_{O} > 0 \mid Y_{j}, yO)$.

The classical approach here would look for the sampling distribution of the estimate of $\theta_{J} - \theta_{O}$, which is intractable without using Normal approximations. However as these samples are clearly independent we just compare the posterior distributions $\theta_{J} \sim Beta(90,12)$ and $\theta_{O} \sim Beta(41,61)$.

Clearly $Pr(\theta_{J} > \theta_{O} \mid Y_{j}, yO) \sim= 0$. (Formal analysis in next week's lab.) Jordan has a better free throw percentage than O'Neal, based on this data. The plot was created using the following R code

```{r}
x<-(0:1000)/1000 
px1<-dbeta(x,41,61) 
px2<-dbeta(x,90,12) 
plot(x,px2,type='l') 
lines(x,px1)
```

## Question 4 (Honours/Postgraduates only)

Let  $Y \sim Binomial (n=10, \theta)$.

##### (i)	
Derive the prior predictive distribution p(y), under a Unif(0,1) prior on \theta, and plot it
 
$$
\begin{aligned}
Pr(Y &= y) = p(y) = \int\limits_{0}^{1}p(y,\theta)d\theta = \int\limits_{0}^{1} p(y \mid \theta)p(\theta)d\theta = \binom{10}{y}\int\limits_{0}^{1}\theta^y(1-\theta)^{10-y}d\theta \\ 
&= \binom{10}{y}\frac{\Gamma(y + 1)\Gamma(11-y)}{\Gamma(12)}=\frac{10!y!(10-y)!}{y!(10-y)!11!} \\ 
&=	1/11; \qquad y = 0,1,2,\cdots,10
\end{aligned}
$$ 

##### (ii)
Derive the posterior predictive density $p(y*\mid y)$ and plot it.
$$
\begin{aligned}
Pr(Y_2 &= x \mid Y_1 = y) = \int\limits_{0}^{1}p(x,\theta\mid Y_1 = y)p(\theta \mid Y_1 = y)d\theta \\
&= \binom{10}{y}\int\limits_{0}^{1}\theta^x(1-\theta)^{10-x}\frac{\Gamma(12)}{\Gamma(y + 1)\Gamma(11-y)}\theta^y(1-\theta)^{10-y}d\theta \\ 
&= \binom{10}{y}\frac{\Gamma(12)}{\Gamma(y + 1)\Gamma(11-y)}\int\limits_{0}^{1}\theta^{x+y}(1-\theta)^{20-x-y}d\theta \\ 
&=\binom{10}{y}\frac{\Gamma(12)}{\Gamma(y + 1)\Gamma(11-y)}\frac{\Gamma(x + y + 1)\Gamma(21-x-y)}{\Gamma(22)}; \qquad x,y = 0,1,2,\cdots,10
\end{aligned}
$$ 

You can program this in R using a function named bb, thus:

```{r}
bb=function(x,y) {
  g=lgamma(11)+lgamma(12)+lgamma(x+y+1)+lgamma(21-x-y)
  g=g-lgamma(x+1)-lgamma(11-x)-lgamma(y+1)-lgamma(11-y)
  g=g-lgamma(22)
  exp(g)
}
```

We can draw the graph for each possible $y$. Here, distributions will be drawn for $y=0, 1, 2, 5, 8, 9, 10$.

```{r}
x <- seq(0,10,1)
y <- c(0,1,2,5,8,9,10)
par(mfrow=c(3,3))
px <- vector(mode='numeric', length = 11)
for(i in 1:length(y)){
  for(k in 1:length(x)){
    px[k] <- bb(x[k],y[i])
  }
    plot(x, px, type = 'l')
}
```