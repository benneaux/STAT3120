---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---

# Lab Exercises 4 Solutions

## Q1 [Classical method revision]																			

#### (a) and (b)	

$$
\begin{aligned}
	f(x_{1},x_{2},\dots x_{n} \mid \mu,\sigma) 
  	&= \prod\limits_{i=1}^{n}
  	  \frac
  	    {1}
  	    {\sigma^{2} \pi}
  	 exp\left(
  	      -\frac
  	         {1}{2}
  	       \left[
  	       \frac
  	         {x_{i}-\mu}
  	         {\sigma}
  	       \right]^{2}
  	    \right) \\
	l(\mu,\sigma) 
	  &= -n\text{log}\sigma 
	     -\frac
	        {n}{2}
	     \text{log}2\pi 
	     -\frac
	        {1}{2\sigma^{2}}
	     \sum(x_{i}-\mu)^{2} \\
  \frac{\delta l}{\delta \mu} 
    &= \frac
        {1}{\sigma^{2}}
       \sum(x_{i} - \mu); \qquad
    \frac
      {\delta l}{\delta \sigma}
     = -\frac
          {n}{\sigma}
       + \sigma^{-3}\sum(x_{i} ' \mu)^{2} \\
  \hat{\mu}_{ML} 
    &= \bar{x}; \qquad 
  \hat{\sigma}_{ML} 
     =\frac
        {1}{n}
      \sum\limits_{i=1}^{n}(x_{i} - \bar{x})^{2}
\end{aligned}
$$


#### (c)
Recall that the sampling distribution of the mean for a given variance is $\frac{\bar{X}- \mu}{\sigma/\sqrt{n}}\sim N(0,1)$

$$
P\left(z_{\alpha / 2}	\leq \frac{\bar{X}- \mu}{\sigma/\sqrt{n}} \leq z_{(1 - \alpha / 2)}\right) = 1 - \alpha \\
P(\bar{X} - z_{(1-\alpha / 2)}\sigma /\sqrt{n} \leq \mu \leq \bar{X} - z_{\alpha / 2)}\sigma /\sqrt{n}) = 1 - \alpha \\
100(1- \alpha ) \% CI : (\bar{X} - z_{(1-\alpha / 2)}\sigma /\sqrt{n} , \bar{X} + z_{(1-\alpha / 2)}\sigma /\sqrt{n})
$$ 

#### (d)
It can also be shown that with $\hat{\sigma} = \sqrt{ \frac{1}{n}\sum\limits_{i=1}^{n}(x_{i} - \bar{x})^{2}}$  , we have a known sampling distribution in	$\frac{n\hat{\sigma}^{2}}{\sigma^{2}}\sim \chi_{n}^{2}$. This is the chi-squared distribution with $n$ degrees of freedom.

Thus:

$$
P\left(\chi_{n}^{2}(\alpha / 2)\leq \frac{n\hat{\sigma}^{2}}{\sigma^{2}} \leq \chi_{n}^{2}(1 - \alpha / 2)\right) = 1 - \alpha \\
P\left(\frac{n\hat{\sigma}^{2}}{\chi_{n}^{2}(1 - \alpha / 2)} \leq \sigma^{2} \leq \frac{n\hat{\sigma}^{2}}{\chi_{n}^{2}(\alpha / 2)}\right) = 1 - \alpha \\
100(1- \alpha ) \% CI : \left(\frac{n\hat{\sigma}^{2}}{\chi_{n}^{2}(1 - \alpha / 2)},\frac{n\hat{\sigma}^{2}}{\chi_{n}^{2}(\alpha / 2)}\right)
$$ 

## Question 2
The data in this question is contained in the file "chicken.txt" (or "stat3120data.xls") which you can download from the Blackboard site. It is taken from question 3, section 3.10 of BDA. Consider the calcium flow data set in this file. The effects of a magnetic field on calcium flow in chicken brains have been measured. Assuming a diffuse prior distribution on the unknown means and that the sample variances equal the true variances:

#### (a) Find point estimates and 95% CrIs for the treatment and control means $(\mu_{T}\> \text{and} \> \mu_{C})$ in the calcium flow example. Plot the posterior densities.

From Lecture 4 we know in general that:

$$
  \mu \mid y, \sigma^{2} \sim N\left(\mu_{1}=\frac{n\bar{y}/\sigma^{2} + \mu_{0}/\sigma_{0}^{2}}{n/\sigma^{2}+1/\sigma_{0}^{2}},\> \sigma_{1}^{2} = \frac{1}{n/\sigma^{2}+1/\sigma_{0}^{2}} \right)
$$  

and that under a diffuse prior $\sigma_{0}^{2} \implies \infty$ , so that $\mu \mid y \sim N (\bar{y},	\sigma^{2}/n)$.	
To input the data, copy and paste from Excel using	

```{r, eval = FALSE}
y_{t} <- scan(file="") or even y_{t}<-scan()
```

followed by enter, paste, and enter (and the same for yc); or alternatively read from a text file:

```{r}
chicken <-readRDS("Labs/LabData/chicken.rds") 
yt <-chicken[1:36,2] # treatment data 
yc <- chicken[1:32,1] # control data
```

The sample means and variances are :

Treatment: $\bar{y}_{T}  = 1.173; s_{T}^{2}  = 0.04$

in R: 

```{r}
mean(yt)
var(yt)
```

Control: $\bar{y}_{C} = 1.013; s_{C}^{2}  = 0.0576$

Thus the posterior densities are treatment: $\mu_{T}  \mid y \sim N (1.173, 0.04 36)$.

And control: $\mu_{C} \mid y_{C}  \sim N (1.103,	0.0576 32)$. These densities are plotted below:

```{r}
x=seq(0.5,1.5,by=0.005)

par(mfrow=c(2,1))
plot(x,dnorm(x,mean(yt),sqrt(var(yt)/length(yt))),type='l')
title('treatment')

plot(x,dnorm(x,mean(yc),sqrt(var(yc)/length(yc))),type='l')
title('control')
```

Point estimates for the unknown means are $\hat{\mu}_{T}  = y_{t}  = 1.173; \muC  = yC  = 1.013$.

95% probability intervals are $\mu_{T}  \pm 1.96	\sigma^{2}		\sigma^{2}	
	T ; \mu_{C} \pm 1.96	C	
	n_{t}	nC$	
In R this is:	

```{r}
mean(yt)+qnorm(0.975)*sqrt(var(yt)/length(yt)) 
mean(yt)-qnorm(0.975)*sqrt(var(yt)/length(yt))
```

This leads to (1.108, 1.238) for the treatment mean and (0.930, 1.096) for the control mean.

#### (b) Use simulation to find the same estimates and CrIs (to 2 decimal places). How long must you run the simulation to achieve this accuracy?

1000 Monte Carlo samples for the treatment mean:

```{r}
mut <- rnorm(1000,mean(yt),sqrt(var(yt)/length(yt)))
```

Find the quantiles using:

```{r}
quantile(mut,c(0.025,0.975))
```


#### (c)	Estimate $Pr(\muΤ > 1 \mid y, \sigma^{2})$ for the treatment group using the exact result and simulation.

$$Pr(\muΤ > 1 \mid y, \sigma2) = 1-Pr(\muΤ < 1 \mid y, \sigma2)$$
```{r}
1-pnorm(1,mean(yt),sqrt(var(yt)/length(yt)))
```
= 0.9999999 using R.

Using an MC sample of 10000 we obtain

```{r}
mut <- rnorm(10000,mean(yt),sqrt(var(yt)/length(yt))) 
length(mut[mut>1])/10000 # or simply mean(mut>1)
```

which gives 1, and so our estimate is >9999/10000 = 0.9999

#### (d) The scientist who collected the data reports that, from physical theory, the mean fluid flow in chicken brains can only be between 0.4 and 1.8. Repeat parts (a) and (c) using this information to construct a suitable prior distribution for the means. Plot your new prior and new posterior. How sensitive are your results to the different priors?

If we allow 6 standard errors between 0.4 and 1.8 this gives a prior variance of $\sigma_0^2 = 0.0544$, with a prior mean of (0.4+1.8)/2 = 1.1. So

$$
\begin{aligned}
\mu_T	\mid y_T	,\sigma^2		
&\sim N\left(\mu_1	=	\frac{36*1.179/0.04 + 1.1/0.0544}{36/0.04 + 1/0.0544}	,\>\sigma_1^2	= \frac{1}{36 / 0.04 + 1/ 0.0544}\right) \\			
&= N (\mu_1	= 1.1715,\>\sigma_1^2	= 0.00109)
\end{aligned}
$$

This is identical to 2 decimal places to that using the diffuse prior. For the control mean we have

$$
\begin{aligned}
\mu_C	\mid y_C	,\sigma^2		
&\sim N\left(\mu_1	=	\frac{32*1.013/0.0576 + 1.1/0.0544}{32/0.0576 + 1/0.0544},\>\sigma_1^2	= \frac{1}{32/0.0576 + 1/0.0544}\right) \\			
&= N (\mu_1	= 1.157,\>\sigma_1^2	= 0.00174)
\end{aligned}						
$$

The posterior is a little tighter than previously. Plots of each prior and posterior are below using R commands.


```{r}
x <- seq(0,2.0,by=0.005)
par(mfrow=c(2,2))

plot(x,dnorm(x,1.1,sqrt(0.0544)),type='l',xlab="Prior", ylab="Dens")
title('Treatment')
plot(x,dnorm(x,1.1715,sqrt(0.00109)),type='l',xlab="Posterior", ylab="Dens")
title('Treatment')
plot(x,dnorm(x,1.1,sqrt(0.0544)),type='l',xlab="Prior", ylab="Dens")
title('Control')
plot(x,dnorm(x,1.0157,sqrt(0.00174)),type='l',xlab="Posterior", ylab="Dens")
title('Control')
```

95% probability intervals are now treatment (1.107, 1.236) and control (0.934, 1.098). These intervals are not sensitive to the different priors we have considered (not shown).

## Question 3
For the football spread data in Lecture 1 (also Section 1.6 in BDA), we have n = 672 differences, d, between the point spread and the actual spread. From these we have a sample

mean of d = 0.07 and a sample variance of s2=13.862.

#### (a) Assuming that $\sigma^2 =142$, find the posterior distribution for the unknown mean difference.

$$
\begin{aligned}
  \mu \mid y, \sigma^{2} 
  &\sim N\left(\mu_{1}=\frac{n\bar{d}/\sigma^{2} + \mu_{0}/\sigma_{0}^{2}}{n/\sigma^{2}+1/\sigma_{0}^{2}},\> \sigma_{1}^{2} = \frac{1}{n/\sigma^{2}+1/\sigma_{0}^{2}} \right)\>\text{, and, when }\>\sigma_0^2 \rightarrow \infty, \\
   \mu \mid y, \sigma^{2} 
  &\sim N\left(\mu_{1}=\bar{d},\> \sigma_{1}^{2} = \frac{\sigma^{2}}{n}\right) 
\end{aligned}
$$																					
Thus, in this case,

$$
\mu \mid y, \sigma^{2} 
\sim N\left(\mu_{1}=0.07,\> \sigma_{1}^{2} = \frac{14^{2}}{672}=0.2917\right) 
$$
#### (b) Find a 90% probability interval for the unknown mean difference, $\mu, and the posterior mean and modal estimates, given that $\sigma^{2}=142$.

$$
\mu \mid y, \sigma^{2} 
\sim N\left(\mu_{1}=0.07,\> \sigma_{1}^{2} = \frac{14^{2}}{672}=0.2917\right)\>\text{ means that a 90% interval is} \\
0.007 \pm z_{0.95}*\sqrt{0.2917} = 0.07\pm\text{qnorm}(0.95,0,1)*\sqrt{0.2917}=(-0.818,0.958).
$$
## Question 4
From Q3 now consider that d $\sim N(0, \sigma^2)$.

#### (a) Find the posterior distribution for the unknown variance parameter. Plot this density.
 
Using the prior $\sigma^2 \mid \mu, y$ where we have

$$
\sigma^{2} \mid \mu ,y  
\sim I.G.\left(A=\frac{n}{2},\> B = \frac{\sum(y_i-\mu)^2}{2}\right), 
$$						
where we have

$$
\sigma^{2} \mid \mu = 0, d  
\sim I.G.\left(A=\frac{672}{2},\> B = \frac{\sum\limits_{i=1}^{672}d_i^2}{2}\right). 
$$

We know that $n=672$, $\bar{d} = 0.07$; $s^2 = 13.862 = \frac{1}{671}\sum(d_i - \bar{d})^2$, and so $671*13.86^2 = \sum d_i^2 - 672 * 0.07^2$ and $\sum d_i^2 = 671*13.86^2 + 672 * 0.07^2 = 128902.12$. Thus, $\sigma^{2} \mid \mu = 0, d \sim I.G.(A = 336, B = 64451.06)$, which is equivalent to $1/\sigma^{2} \mid \mu = 0, d \sim Gamma(A = 336, B = 64451.06)$.

We can use the function code in Lecture 4,

```{r}
invgamm<-function(x,a,b) {
  lg=a*log(b)-lgamma(a)-(a+1)*log(x)-b/x 
  return(exp(lg))
} 
```

followed by

```{r}
A <- 336
B <- 64451.06
x <- seq(100,300,by=0.01)
plot(x,invgamm(x,A,B),type='l')
```

#### (b) Find a 90% CrI for the unknown variance parameter, $\sigma^2$, and estimates of the posterior mean and mode.

##### (1)	Find a 90% interval for a $Gamma(336, 64451.06)$ and invert it:

```{r}
1/qgamma(c(0.95,0.05),A,B)
```

which gives $(175.7556, 210.3299)$; alternatively,

##### (2) simulate an MC sample from a $Gamma(336, 64451.06)$ and invert the sample 

```{r}
g <- rgamma(10000,A,B)
quantile(1/g,c(0.05,0.95))
```

##### (3) which gives $(176.1946, 210.7591)$. (Note: fairly noisy even for $10^6$ samples!) 

```{r}
mean <- B/(A-1)
mode <- B/(A+1)
```

#### (c) A betting expert from Centrebet suggests to you that it is virtually impossible for the variance of *d* to go below 160. Use this information to construct a suitable prior for $\sigma^{2}$ and then find the new posterior. How sensitive are your results to the different priors?

One approach would be to use the same prior density, but restrict it to be 0 when $\sigma^2<160$. This
basically means that we have the same *posterior* too, but again restricted to 0 for $\sigma^2<160$. This is plotted below.


It looks the same because the probability
$$
\begin{aligned}
Pr(\sigma^{2} < 160 \mid \mu = 0, d ) &= Pr(I.G.(A = 336, B = 64451.06) < 160) \\
&= Pr(Gamma(A = 336, B = 64451.06) > 1/160)
\end{aligned}
$$

is $1-Pr(Gamma(1/160,336, 64451.06)) = 1 - 0.99972 = 0.00028$: almost all of the posterior distribution is above 160 anyway. We could also take an MC sample and reject any sampled points that were below $160$: 

```{r}
g <- rgamma(10000,336,64451.06)
g <- g[g<1/160]
quantile(1/g,c(0.05,0.95))
```

which gives $(176.1973, 210.7595)$, which is basically unchanged. (One point was deleted here!)

End of Lab 4.