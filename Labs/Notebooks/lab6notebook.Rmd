---
title: "Lab Exercises 6 Solutions"
output:
  html_document: default
  html_notebook: default
---
```{r setup, include = FALSE}
require(knitr)
require(broom)
```

## Question 1 
The table below is taken from BDA. It contains results of a survey on bicycle traffic around Berkeley, CA in 1993. From the table in BDA we consider only the first four rows of data for residential streets. This consists of bicycle counts on a particular day on different streets around Berkeley. Also recorded is whether the street has a certified bicycle lane or not.
```{r, echo = FALSE}
cnames <- c("Bike route?","Bicycle count","Bike route?","Bicycle count")
data <- matrix(c(rep("Yes",9), 16,9,10,13,19,20,18,17,35,"Yes",rep("No",8),55,12,1,2,4,9,7,9,8),9,4,byrow=FALSE)
colnames(data) <- cnames
kable(data)
```

*We wish to compare the amount of bicycle traffic on the sections of road that are designated bike routes, with that for roads without bike routes. Type the data in R:*

```{r}
y <- c(16,9,10,13,19,20,18,17,35,55) # for bike route 
z <- c(12,1,2,4,9,7,9,8) # for non-bike route
```

#### (a) Consider the group bike route ='Yes' (Y) first. What distribution are the bicycle counts most likely to follow?

Assuming the counts are independent and identically distributed, we would use a Poisson distribution here. So, $Y \sim Poisson (\theta_{Y})$

#### (b) Assume that the bicycle counts on bike routes are independent and identically distributed for different streets. What is the conditional likelihood formula for the counts Y?

 
$$
p(y \mid \theta_{Y}) = \prod \frac{\theta_{Y}^{y_i} exp(-\theta_{Y})}{y_i!} =	\frac{\theta_{Y}^{n\bar{y}} exp(-n\theta_{Y})}{\prod y_i!}\propto \theta_{Y}^{212}exp(-10\theta_{Y})
$$

This is in the form of a $Gamma(213,10)$ density in $\theta_{Y}$.

#### (c) What prior information do we have? Construct a prior distribution for $\theta_{Y}$ the parameter of interest for the counts y.

In this case we seem to have no prior information about $\theta_{Y}$. Our preferred noninformative prior is $Gamma(1,0)$.

#### (d)	Update the prior with the observed counts to find the posterior distribution for $\theta_{Y} \mid y$. Generate an estimate and 95% interval for $\theta_{Y} \mid y$.

The posterior for $\theta_{Y} \mid y$ is $Gamma (213, 10)$. $E(\theta_{Y} \mid y) = 213/10 = 21.3$.

A 95% interval is $(`r qgamma(c(0.025,0.975),213,10)`)$

#### (e)  Let Z = bicycle counts on non-bike routes. How can we compare Y and Z?

Naturally, we compare $\theta_{Y}$ and $\theta_{Z}$, the rates of bicycle usage for bike routes and non-bike routes.

#### (f)	Estimate a 95% confidence interval on the difference $\theta_{Y} – \theta_{Z}$. What conclusions can you draw based on this interval?

Rather than attempt to find the true distribution of $\theta_{Y} – \theta_{Z} \mid y,z$ we can use Monte Carlo simulation here. Just simulate from $\theta_{Y} \mid y \sim Gamma(213,10)$ and $\theta_{Z} \mid z \sim Gamma (53, 8)$. Then simply subtract each iterate of $\theta_{Z}$ from each iterate of $\theta_{Y}$ obtaining a sample from $\theta_{Y} – \theta_{Z} \mid y,z$.

i.e. 

```{r}
ty <- rgamma(10000,213,10) 
tz <- rgamma(10000,53,8) 
tymz <- ty-tz
```

The quantile function is then used as follows 

```{r}
quantile(tymz,c(0.025,0.975))
```

A point estimate is
```{r}
mean(tymz)
```

A histogram follows
```{r}
hist(tymz)
```


Clearly bike routes are used significantly more than non-bike routes.

#### (g) Estimate the probability that $\theta_{Y} – \theta_{Z}$ is greater than 10 given the data.

Using the sample in (f)

```{r}
length(tymz[tymz>10])/length(tymz) # (or: mean(tymz>10)!)
```

We are very confident that bike usage is at least 10 per day per street more on bike routes than non-bike routes.

#### (h)  Perform the equivalent t-test. Discuss why inference appears different from (g).

```{r}
model_summary <- tidy(t.test(y,z,"greater",10))
model_summary
```

gives a p-value of `r model_summary$p.value`. This would need to be compared with $0.003$ based on (g)! In hindsight, there is a problem with the Poisson assumption (rather than the t-test, in our opinion, as it's very robust). E.g. mean(y) = `mean(y)` and $var(y) = `var(y)`: there seems to be overdispersion.

## Question 2 
#### (a) The likelihood contributes a $Gamma (213, 10)$ to the posterior.

A $Gamma(a,b)$ prior means the posterior is $\theta_{Y} \mid y \sim Gamma (212+a, 10+b)$. 
So we have $Gamma(0,0)$ prior $ \theta_{Y} \mid y \sim Gamma (212, 10)$ and $Gamma(0.5,0)$ prior $ \theta_{Y} \mid y \sim Gamma (212.5, 10)$.
95% intervals for these are (using $m=10,000,000$ – even then not always enough for 2 dps!)

```{r, echo = FALSE}
cnames <- c("Prior","Posterior","95% interval \u03B8_y|y","95% interval \u03B8_y - \u03B8_z|y,z","Pr(\u03B8_y - \u03B8_z > 10)")
data <- matrix(c("0,0",	"212,10",	"18.44, 24.15",	"11.38, 18.09",	"0.997",	"0.5,0",	"212.5,10",	"18.49, 24.20",	"11.36, 18.09",	"0.997", "1,0",	"213,10",	"18.54, 24.25",	"11.33, 18.08",	"0.997"),nrow = 3,ncol=5,byrow = TRUE)
colnames(data) <- cnames
kable(data, escape = FALSE)
```
 
Results seem quite insensitive to these prior choices.

R code for Gamma(0,0) 

```{r}
qgamma(c(0.025,0.975),212,10)
ty1 <- rgamma(10000,212,10)
tz1 <- rgamma(10000,52,8) 
tymz1 <- ty1-tz1
```

The quantile function is then used as follows

```{r}
quantile(tymz1,c(0.025,0.975)) ##

length(tymz1[tymz1>10])/length(tymz1)	###
```

For Gamma(0.5,0) 

```{r}
qgamma(c(0.025,0.975),212.5,10)
ty2 <- rgamma(10000,212.5,10)
tz2 <- rgamma(10000,52.5,8) 
tymz2 <- ty2-tz2
```

The quantile function is then used as follows 

```{r}
quantile(tymz2, c(0.025,0.975))
length(tymz2[tymz2>10])/length(tymz2)
```
***
#### (b) Under a $Gamma(20,2)$ prior $\theta_{Y} \mid y \sim Gamma (232, 12)$ and under a $Gamma(20,3)$ prior $\theta_{Z} \mid z \sim Gamma (72, 11)$

For informative priors $Gamma(20,2)$ for $Y$ and $Gamma(20,3)$ for $Z$

For Y:

```{r}
qgamma(c(0.025,0.975),232,12)
ty3 <- rgamma(10000,232,12)
tz3 <- rgamma(10000,72,11) 
tymz3 <- ty3-tz3
```

The quantile function is then used as follows:

```{r}
quantile(tymz3, c(0.025,0.975))
length(tymz3[tymz3>10])/length(tymz3)
```
 
The table now is
```{r, echo = FALSE}
cnames <- c("Prior","Posterior","95% interval \u03B8_y|y","95% interval \u03B8_y - \u03B8_z|y,z","Pr(\u03B8_y - \u03B8_z > 10)")
data <- matrix(c("0,0",	"212,10",	"18.44, 24.15",	"11.38, 18.09",	"0.997",	"0.5,0",	"212.5,10",	"18.49, 24.20",	"11.36, 18.09",	"0.997", "1,0",	"213,10",	"18.54, 24.25",	"11.33, 18.08",	"0.997","20,2",	"232, 12",	"16.93, 21.90",	"9.91, 15.74",	"0.971", "20,3",	"72, 11","","",""),nrow = 5,ncol=5,byrow = TRUE)
colnames(data) <- cnames
kable(data, escape = FALSE)
```

		

Results have changed somewhat with the expert prior. The prior on $\theta_{Y}$ has a mean at 20/2 = 10 which is well below the mean of y (21.3), which explains why the posterior has been shrunk towards 0.


The posteriors for $\theta_{Y} \mid y$ are shown below. The blue line is the $Gamma(232,12)$ from the expert prior. The posteriors based on the 3 candidate noninformative priors are almost indistinguishable.

```{r, echo=FALSE}
x <- seq(15,35,0.01)
plot(x, dgamma(x,213,10),type="l",ylim=c(0,0.35), ylab="Density",xlab="") 
curve(dgamma(x,212,10),col="red", add=TRUE) 
curve(dgamma(x,212.5,10),col="green", add=TRUE) 
curve(dgamma(x,232,12),col="blue", add=TRUE)
```
